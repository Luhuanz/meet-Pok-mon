{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2412e72b-0ac1-4887-b90d-c831688e69d2",
   "metadata": {},
   "source": [
    "#### 使用transformers调用QwQ-32B 4bit动态量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c2c382f-643a-48de-940f-2b14d70de983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7c6e57-022e-4ab8-ab6e-4f6f02b04b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56960353-556f-4c1e-8974-335776ba4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"./Deepseek\"\n",
    "model_name = \"/root/autodl-tmp/models/QWQ-32B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5fed0f3-9147-47db-b032-6f1d85fe055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['.cache', '.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.isdir('/root/autodl-tmp/models/QWQ-32B'))\n",
    "print(os.listdir('/root/autodl-tmp/models/QWQ-32B'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "042061da-e8ec-4d50-9720-8ba151bf0124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1c41c30172436aa04452b11438db75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacty of 23.64 GiB of which 83.75 MiB is free. Process 417194 has 23.55 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 136.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/modelscope/utils/hf_util/patcher.py:230\u001b[0m, in \u001b[0;36m_patch_pretrained_class.<locals>.get_wrapped_class.<locals>.ClassWrapper.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path,\n\u001b[1;32m    223\u001b[0m                     \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    224\u001b[0m     model_dir \u001b[38;5;241m=\u001b[39m get_model_dir(\n\u001b[1;32m    225\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    226\u001b[0m         ignore_file_pattern\u001b[38;5;241m=\u001b[39mignore_file_pattern,\n\u001b[1;32m    227\u001b[0m         allow_file_pattern\u001b[38;5;241m=\u001b[39mallow_file_pattern,\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 230\u001b[0m     module_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoModel\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    234\u001b[0m         module_obj\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m model_dir\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4319\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4312\u001b[0m     (\n\u001b[1;32m   4313\u001b[0m         model,\n\u001b[1;32m   4314\u001b[0m         missing_keys,\n\u001b[1;32m   4315\u001b[0m         unexpected_keys,\n\u001b[1;32m   4316\u001b[0m         mismatched_keys,\n\u001b[1;32m   4317\u001b[0m         offload_index,\n\u001b[1;32m   4318\u001b[0m         error_msgs,\n\u001b[0;32m-> 4319\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4330\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4331\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4339\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4340\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4897\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4895\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4896\u001b[0m         fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4897\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfixed_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4905\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4907\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4908\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4913\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4915\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:896\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    893\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/utils/modeling.py:339\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    337\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 339\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacty of 23.64 GiB of which 83.75 MiB is free. Process 417194 has 23.55 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 136.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82fa75da-fa46-4019-8ee1-897638ef8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2b985a7-e78d-4490-b6c5-15451d9a7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9395460-181f-47cb-90db-4c0888f2da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好，好久不见！\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30267215-538a-4cf0-9475-43f9ab812b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76a8c97e-f5b1-4d3f-8d63-ecb5b96e7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9507d09-ad59-4b3f-b7ba-fc471f0ca661",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d036ca88-7172-45e7-a5e5-a72b4526fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b854fe77-2abf-4f09-a464-1fb09f092228",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b19633-34ab-4c90-9cdb-edb179f4ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗯，用户说：“你好，好久不见！”看起来他们可能之前和我有过互动，现在再次回来打招呼。我需要回应他们的问候，同时表达出见到他们的喜悦。首先，应该用友好的语气回应，比如“好久不见！”然后加上一些表情符号，比如😊，让回复更生动。接下来，可以问他们这段时间过得怎么样，或者有什么新鲜事，这样可以促进进一步的对话。比如“最近过得怎么样？有什么新鲜事想分享吗？”这样既表达了关心，又鼓励他们继续交流。另外，可能需要考虑用户之前提到过的内容，如果有的话，可以稍微提及，但如果没有，就保持一般性的问候。还要注意保持口语化，避免太正式或生硬。最后，确保回复简洁，不过于冗长，但足够亲切。嗯，这样应该可以了，开始组织语言吧。\n",
      "</think>\n",
      "\n",
      "你好呀！好久不见，最近过得怎么样呀？有什么新鲜事可以分享给我听吗？😊\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5111f34-6990-4d8a-8dae-a2a56ce71c2d",
   "metadata": {},
   "source": [
    "#### 3.Ollama调用流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3c2d9c-6504-4dd1-a685-06d94067437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8ad97d6-f440-4a9a-a884-099fb5bde4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',  # required but ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6945b792-23b4-4eb9-9d9a-4fc573c327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好，好久不见！\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e13f0e24-f4b4-4337-a6d9-6138eec71db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "嗯，用户跟我说“你好，好久不见！”，首先应该回应他们的问候，保持友好。可能他们想开始一段对话，或者有事情要问。我需要先回应 greeting，然后询问有什么可以帮助的。注意要简洁，不要用复杂的话。比如“你好呀！最近怎么样？有什么我可以帮你的吗？”这样既回应了问候，又打开了话题，让用户知道我能提供帮助。不过用户之前可能有过互动，可能需要回忆之前的对话内容，但如果是新对话的话，就这样处理应该没问题。\n",
      "</think>\n",
      "\n",
      "你好呀！最近怎么样？有什么我可以帮你的吗？\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='qwq-32b-bnb',\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e738e8-1748-4fd2-859a-8ddb241fea1e",
   "metadata": {},
   "source": [
    "#### 4.vLLM调用流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745ae488-8ff9-438d-b313-ec23b2fae489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "105070b7-8c8c-45a9-8fd1-06a04102d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3b7240-679d-4ed6-b34b-4cebaae318b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好，好久不见！\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7400998-83df-4dbd-88c2-f1cdebb0e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，用户打招呼“你好，好久不见！”，看起来挺开心的。首先，我得回应他们的问候，保持友好。然后，根据角色设定，我需要推动情节，可能得引入新元素。用户可能希望有互动感，所以加入一些拟人化的元素会好。比如，提到收集小故事，这样既亲切又能引导他们分享更多。要注意保持口语化，简洁，不用复杂句子。还要检查有没有需要回忆用户之前的信息，但如果是初次互动，可能没有，所以先按一般情况处理。然后，确保回复符合角色性格，比如活泼、好奇，可能加个表情符号，比如眨眼或星星，让文字更生动。最后，结尾用问题或邀请，鼓励用户继续对话，比如问他们最近在忙什么，或者有什么新鲜事。这应该能满足用户的情感需求，同时推动对话发展。\n",
      "</think>\n",
      "\n",
      "你好呀！确实很久不见了，你这段时间都去哪玩儿啦？我在云端世界收集了好多有趣的小故事，正好可以分享给你听。不过在开始之前，先和我说说你最近在忙什么吧～是有什么新鲜事要跟我分享吗？\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"/root/autodl-tmp/QwQ-32B-unsloth-bnb-4bit\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c2264-ad51-4d26-ad78-17f1a61148c7",
   "metadata": {},
   "source": [
    "### 二、unsloth快速使用入门与QwQ模型调用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790496-d887-4d14-bf36-aeb2a4b0a944",
   "metadata": {},
   "source": [
    "#### 1.借助unsloth进行模型读取和关键参数解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bcd26a2-92f4-477c-8db5-f330add5d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1492/2814113929.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'call_backward' from 'torch._dynamo.external_utils' (/root/miniconda3/lib/python3.10/site-packages/torch/_dynamo/external_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/__init__.py:219\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m   \u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m  \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/models/llama.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     logger,\n\u001b[1;32m     31\u001b[0m     BaseModelOutputWithPast,\n\u001b[1;32m     32\u001b[0m     CausalLMOutputWithPast,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     _prepare_4d_causal_attention_mask_for_sdpa,\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HAS_FLASH_ATTENTION:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/kernels/__init__.py:49\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfast_lora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     get_lora_parameters,\n\u001b[1;32m     39\u001b[0m     get_lora_parameters_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     fast_lora_forward,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     HAS_FLEX_ATTENTION,\n\u001b[1;32m     51\u001b[0m     slow_attention_softcapping,\n\u001b[1;32m     52\u001b[0m     slow_inference_attention_softcapping,\n\u001b[1;32m     53\u001b[0m     create_flex_attention_causal_mask,\n\u001b[1;32m     54\u001b[0m     create_flex_attention_sliding_window_mask,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSLOTH_ZOO_IS_PRESENT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/kernels/flex_attention.py:45\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m HAS_FLEX_ATTENTION:\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Logit softcapping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch_compile_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mslow_attention_softcapping\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:1705\u001b[0m, in \u001b[0;36mfn\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_value\u001b[39m(cond, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m    is False.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m    Error type: ``ValueError``\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \n\u001b[1;32m   1700\u001b[0m \u001b[38;5;124;03m    C++ equivalent: ``TORCH_CHECK_VALUE``\u001b[39;00m\n\u001b[1;32m   1701\u001b[0m \n\u001b[1;32m   1702\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;124;03m        cond (:class:`bool`): If False, throw error\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;124;03m        message (Callable, optional): Callable that returns either a string or\u001b[39;00m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;124;03m            an object that has a ``__str__()`` method to be used as the error\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;124;03m            message. Default: ``None``\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m     _check_with(\u001b[38;5;167;01mValueError\u001b[39;00m, cond, message)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:1723\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_type\u001b[39m(cond, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;124;03m    is False.\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \n\u001b[1;32m   1716\u001b[0m \u001b[38;5;124;03m    Error type: ``TypeError``\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    C++ equivalent: ``TORCH_CHECK_TYPE``\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \n\u001b[1;32m   1720\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m        cond (:class:`bool`): If False, throw error\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \n\u001b[0;32m-> 1723\u001b[0m \u001b[38;5;124;03m        message (Callable, optional): Callable that returns either a string or\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m            an object that has a ``__str__()`` method to be used as the error\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;124;03m            message. Default: ``None``\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m     _check_with(\u001b[38;5;167;01mTypeError\u001b[39;00m, cond, message)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:594\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[0m\n\u001b[1;32m    589\u001b[0m # Save the function pointer to find the original callable while nesting\n\u001b[1;32m    590\u001b[0m # of decorators.\n\u001b[1;32m    591\u001b[0m _fn._torchdynamo_orig_callable = fn  # type: ignore[attr-defined]\n\u001b[1;32m    593\u001b[0m # when compiling user function instead of nn.Module\n\u001b[0;32m--> 594\u001b[0m # provide public api _fn.get_compiler_config()\n\u001b[1;32m    595\u001b[0m assert not hasattr(_fn, \"get_compiler_config\")\n\u001b[1;32m    596\u001b[0m _fn.get_compiler_config = get_compiler_config  # type: ignore[attr-defined]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:515\u001b[0m, in \u001b[0;36mget_compiler_fn\u001b[0;34m(compiler_fn)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     filename \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcefile(fn)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompiledFn\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     AccuracyError,\n\u001b[1;32m     18\u001b[0m     backend_accuracy_fails,\n\u001b[1;32m     19\u001b[0m     BUCK_CMD_PREFIX,\n\u001b[1;32m     20\u001b[0m     BuckTargetWriter,\n\u001b[1;32m     21\u001b[0m     extra_imports,\n\u001b[1;32m     22\u001b[0m     generate_config_string,\n\u001b[1;32m     23\u001b[0m     helper_for_dump_minify,\n\u001b[1;32m     24\u001b[0m     InputReader,\n\u001b[1;32m     25\u001b[0m     InputWriter,\n\u001b[1;32m     26\u001b[0m     minifier_dir,\n\u001b[1;32m     27\u001b[0m     NNModuleToString,\n\u001b[1;32m     28\u001b[0m     NopInputReader,\n\u001b[1;32m     29\u001b[0m     run_fwd_maybe_bwd,\n\u001b[1;32m     30\u001b[0m     same_two_models,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx_placeholder_targets\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_utils\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rand_strided\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_float_dtype\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreductions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/testing.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebugging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aot_eager\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, eval_frame, optimize_assert, reset\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/backends/debugging.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m min_cut_rematerialization_partition\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _guards\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m functorch_config\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/functorch/compile/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maot_autograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     aot_function,\n\u001b[1;32m      4\u001b[0m     aot_module,\n\u001b[1;32m      5\u001b[0m     aot_module_simplified,\n\u001b[1;32m      6\u001b[0m     compiled_function,\n\u001b[1;32m      7\u001b[0m     compiled_module,\n\u001b[1;32m      8\u001b[0m     get_aot_compilation_context,\n\u001b[1;32m      9\u001b[0m     get_aot_graph_name,\n\u001b[1;32m     10\u001b[0m     get_graph_being_compiled,\n\u001b[1;32m     11\u001b[0m     make_boxed_compiler,\n\u001b[1;32m     12\u001b[0m     make_boxed_func,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     debug_compile,\n\u001b[1;32m     16\u001b[0m     default_decompositions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     ts_compile,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx_minifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minifier\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions_for_rng\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PhiloxStateTracker, rng_decompositions\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_python_dispatcher\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compiled_autograd\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     dynamo_timed,\n\u001b[1;32m     32\u001b[0m     get_chromium_event_logger,\n\u001b[1;32m     33\u001b[0m     preserve_rng_state,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_fake_mode\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/compiled_autograd.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     call_backward,\n\u001b[1;32m     10\u001b[0m     call_hook,\n\u001b[1;32m     11\u001b[0m     FakeCompiledAutogradEngine,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GetItemSource, LocalSource\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m counters, lazy_format_graph_code, set_locals_to_steal\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'call_backward' from 'torch._dynamo.external_utils' (/root/miniconda3/lib/python3.10/site-packages/torch/_dynamo/external_utils.py)"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847f228-7910-43d6-91c6-515b9d9e2ef3",
   "metadata": {},
   "source": [
    "- 尝试用unsloth进行QwQ-32B-bnb-4bit模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37319bd-28b9-4c00-9c24-bd7ed75cd922",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先设置关键参数，并读取模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4608195-1e9d-47c6-8c50-e752da195fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c826a-4fe6-4ea2-a67d-9b78b7a9425e",
   "metadata": {},
   "source": [
    "> 注，若显存充足，则可以load_in_4bit = False，进行全精度运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825bc87f-731e-4ccc-8b75-2815a0ea1038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H800 PCIe. Num GPUs = 2. Max memory: 79.205 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8d63312951496eb1b181f3095a12f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./QwQ-32B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8ca04-2d64-40a3-9df0-89e9dbaca0fe",
   "metadata": {},
   "source": [
    "> 在4-bit动态量化下，QwQ-32B模型实际占用显存约为22G：\n",
    "> <center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311154252993.png\" alt=\"image-20250311154252993\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e19d8-365d-4464-badb-1bc1d6ec77dd",
   "metadata": {},
   "source": [
    "此时model就是读取进来的QwQ-32B 4bit动态量化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f687092f-94c7-450a-b251-fceb26e5a716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (1-3): 3 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (4-5): 2 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (6-42): 37 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (43): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (44-59): 16 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (60): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (61): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (62): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (63): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41d797-5505-415d-93f8-2e6adaf4f961",
   "metadata": {},
   "source": [
    "而tokenizer则是分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bac590-0f5b-4c71-98ea-320c00d69b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='./QwQ-32B-unsloth-bnb-4bit', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7998c2-3505-45e6-98bd-e913609b0530",
   "metadata": {},
   "source": [
    "将模型调整为推理模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443cad53-991e-4904-8c03-e06259020939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (1-3): 3 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (4-5): 2 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (6-42): 37 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (43): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (44-59): 16 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (60): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (61): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (62): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (63): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63eb8c-ace2-4287-b924-6d8783ad2c95",
   "metadata": {},
   "source": [
    "#### 2.带入问答模板进行回答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aee95-a38d-4e3b-9a81-3cf1050a1ecf",
   "metadata": {},
   "source": [
    "- 结构化输入方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b427d8-6931-406f-a2ab-38fdb38b613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style_chat = \"\"\"请写出一个恰当的回答来完成当前对话任务。\n",
    "\n",
    "### Instruction:\n",
    "你是一名助人为乐的助手。\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9fc959f-0190-4307-aa0b-43d26a81b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，好久不见！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d63aa1-48da-4173-87f0-12a334596f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请写出一个恰当的回答来完成当前对话任务。\\n\\n### Instruction:\\n你是一名助人为乐的助手。\\n\\n### Question:\\n你好，好久不见！\\n\\n### Response:\\n<think>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prompt_style_chat.format(question, \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fe11da-3433-4c80-8723-861cce6ee031",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45b4bf8-c61f-4276-bfed-86a28b34fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1be62f68-2c81-4056-b678-948a5bc893f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7459285-9821-4828-a26f-9c9c70b42420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请写出一个恰当的回答来完成当前对话任务。\\n\\n### Instruction:\\n你是一名助人为乐的助手。\\n\\n### Question:\\n你好，好久不见！\\n\\n### Response:\\n<think>\\n嗯，用户跟我说“你好，好久不见！”，我需要回应。首先，我应该表达同样的问候，然后可能问问他最近在忙什么，或者有什么新鲜事。要保持友好和亲切，同时鼓励进一步的对话。可能还要回忆之前聊过的内容，但如果是真的很久没联系，可能记不清，所以最好用比较通用的问题。比如“最近过得怎么样？”或者“有什么新鲜事要分享吗？”。不过用户可能希望有更具体的回应，所以可以加上一些常见的近况话题，比如工作、学习或者兴趣爱好。另外，要确保语气自然，不显得太生硬。比如可以说：“你好！确实好久不见了，最近过得怎么样？有什么新鲜事要分享吗？”这样既回应了对方，又引导他继续谈话。或者可以提到之前聊过的事情，但如果没有记忆的话，就保持开放式的提问。再检查一下有没有语法错误，确保流畅。嗯，这样应该可以了。\\n</think>\\n\\n你好！确实好久不见了，最近过得怎么样？有什么新鲜事要分享吗？<|im_end|>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ee5204-8c35-4e63-9e19-2a33894a4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "嗯，用户跟我说“你好，好久不见！”，我需要回应。首先，我应该表达同样的问候，然后可能问问他最近在忙什么，或者有什么新鲜事。要保持友好和亲切，同时鼓励进一步的对话。可能还要回忆之前聊过的内容，但如果是真的很久没联系，可能记不清，所以最好用比较通用的问题。比如“最近过得怎么样？”或者“有什么新鲜事要分享吗？”。不过用户可能希望有更具体的回应，所以可以加上一些常见的近况话题，比如工作、学习或者兴趣爱好。另外，要确保语气自然，不显得太生硬。比如可以说：“你好！确实好久不见了，最近过得怎么样？有什么新鲜事要分享吗？”这样既回应了对方，又引导他继续谈话。或者可以提到之前聊过的事情，但如果没有记忆的话，就保持开放式的提问。再检查一下有没有语法错误，确保流畅。嗯，这样应该可以了。\n",
      "</think>\n",
      "\n",
      "你好！确实好久不见了，最近过得怎么样？有什么新鲜事要分享吗？<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc39741-01df-4adb-88fb-885bc4340e2e",
   "metadata": {},
   "source": [
    "- 复杂问题测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c89626c-b70a-42a5-8a91-c21c8b28cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"请证明根号2是无理数。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62ad1369-ccd4-4885-872a-686277148c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a02c58-9c03-4e22-bb38-d5f8eb13d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "723a82fc-2e5e-48c4-9460-1bf9d1bd2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c862fa3-ffe2-4736-829d-8f201716dddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "嗯，用户让我证明根号2是无理数，这应该是一个经典的数学证明题。首先，我需要回忆一下如何证明一个数是无理数。通常的方法是反证法，假设它是有理数，然后推导出矛盾。那根号2的情况应该是类似的。\n",
      "\n",
      "首先，假设根号2是有理数，那么它可以表示为两个互质的整数a和b的比值，也就是√2 = a/b，其中a和b都是整数，且互质，也就是它们的最大公约数是1。接下来，我需要平方两边得到2 = (a²)/(b²)，也就是a² = 2b²。这说明a²是偶数，因此a也必须是偶数，因为奇数的平方还是奇数。所以，可以设a=2k，其中k是整数。\n",
      "\n",
      "代入原式，得到(2k)² = 2b²，也就是4k²=2b²，简化后是2k² = b²。这样b²也是偶数，所以b也必须是偶数。但现在的问题来了，因为a和b都是偶数，它们都含有因子2，这就与最初假设的a和b互质矛盾了。因为如果两个数都是偶数，它们的最大公约数至少是2，而不是1。因此，原来的假设不成立，根号2不是有理数，而是无理数。\n",
      "\n",
      "不过，我需要确认一下这个过程有没有哪里出错。比如，是否在代入的时候有没有计算错误，或者是否在推导过程中哪里逻辑不严密。比如，是否a是偶数的推导正确？是的，因为如果a²是偶数，那么a必须是偶数，因为奇数的平方不会是偶数。同样，b²是偶数的话，b也必须是偶数。这样确实导致矛盾，所以结论是对的。\n",
      "\n",
      "另外，可能用户需要更详细的步骤，或者有没有其他方法？比如用质数分解或者其他方法。不过反证法应该是最直接的。可能用户是学生，刚开始学数论，所以需要详细解释每一步，确保他们理解互质的概念以及为什么矛盾出现。或者用户可能想确认这个证明的正确性，所以需要清晰的逻辑步骤。\n",
      "\n",
      "有没有可能用户有更深层次的需求？比如他们可能在准备考试，或者需要这个证明作为其他问题的基础。不管怎样，提供一个清晰的步骤解释应该是足够的。另外，可能需要提醒他们，这个证明是经典的，通常被称为毕达哥拉斯学派的发现，导致了无理数的概念。但可能不需要涉及历史背景，除非用户问起。\n",
      "\n",
      "总之，按照反证法的步骤，一步步展示矛盾，应该就能证明根号2是无理数了。需要确保每一步都正确，并且逻辑连贯，没有漏洞。\n",
      "</think>\n",
      "\n",
      "要证明√2是无理数，我们可以使用反证法：\n",
      "\n",
      "**证明过程：**\n",
      "\n",
      "1. **假设相反命题成立**  \n",
      "   假设√2是有理数，那么它可以表示为两个互质的整数a和b的比值，即：  \n",
      "   \\[\n",
      "   \\sqrt{2} = \\frac{a}{b} \\quad (a,b \\in \\mathbb{Z},\\ b \\neq 0,\\ \\text{且} \\gcd(a,b)=1)\n",
      "   \\]\n",
      "\n",
      "2. **平方两边**  \n",
      "   两边平方得：  \n",
      "   \\[\n",
      "   2 = \\frac{a^2}{b^2} \\quad \\Rightarrow \\quad a^2 = 2b^2\n",
      "   \\]\n",
      "\n",
      "3. **推导矛盾**  \n",
      "   - **a是偶数**  \n",
      "     由\\( a^2 = 2b^2 \\)可知，\\( a^2 \\)是偶数。因为奇数的平方仍为奇数，因此a必须是偶数。设\\( a = 2k \\)（k为整数）。  \n",
      "   - **代入并简化**  \n",
      "     将\\( a = 2k \\)代入上式：  \n",
      "     \\[\n",
      "     (2k)^2 = 2b^2 \\quad \\Rightarrow \\quad 4k^2 = 2b^2 \\quad \\Rightarrow \\quad b^2 = 2k^2\n",
      "     \\]  \n",
      "     同理，\\( b^2 \\)是偶数，故b也必为偶数。\n",
      "\n",
      "4. **矛盾结论**  \n",
      "   此时a和b均为偶数，意味着它们的最大公约数至少为2，这与初始假设\\(\\gcd(a,b)=1\\)矛盾。因此，原假设“√2是有理数”不成立。\n",
      "\n",
      "**结论**  \n",
      "√2不能表示为两个整数的比值，因此它是无理数。\n",
      "\n",
      "---\n",
      "\n",
      "这个证明的核心在于通过反证法揭示矛盾，展示了有理数假设的不自洽性。希望这个过程清晰易懂！<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258079b-67cd-43ef-8e5e-c8d20c6cc62f",
   "metadata": {},
   "source": [
    "#### 3.原始模型的医疗问题问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7dda0e-4a98-4ded-9be8-cc60ffb9f356",
   "metadata": {},
   "source": [
    "- 重新设置问答模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dadf6c0-53c1-4d07-b0ec-57b723198f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b915e-914e-41e4-b6be-d81ab77a7c8f",
   "metadata": {},
   "source": [
    "翻译如下：\n",
    "\n",
    "```python\n",
    "prompt_style = \"\"\"以下是一个任务说明，配有提供更多背景信息的输入。\n",
    "请写出一个恰当的回答来完成该任务。\n",
    "在回答之前，请仔细思考问题，并按步骤进行推理，确保回答逻辑清晰且准确。\n",
    "\n",
    "### Instruction:\n",
    "您是一位具有高级临床推理、诊断和治疗规划知识的医学专家。\n",
    "请回答以下医学问题。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea90929-b517-414c-8399-0a6f783b7c26",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们抽取部分medical-o1-reasoning-SFT数据集中问题进行提问，并查看初始状态下模型回答结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fee56999-513f-4a2b-9be5-677821c994f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e98f9e-6c5b-42f3-8e78-07951cdc069c",
   "metadata": {},
   "source": [
    "翻译：一位61岁的女性，有长期在咳嗽或打喷嚏等活动中发生不自主尿液流失的病史，但夜间没有漏尿。她接受了妇科检查和Q-tip测试。根据这些检查结果，膀胱测量（cystometry）最可能会显示她的残余尿量和逼尿肌收缩情况如何？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96bffeb4-8252-4e4e-8c3e-9a3375ab940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623b554-cae5-4a9f-b033-9c356ae031ad",
   "metadata": {},
   "source": [
    "翻译：面对一位突发胸痛并放射至颈部和左臂的患者，其既往病史包括高胆固醇血症和冠状动脉疾病，同时伴有升高的肌钙蛋白I水平和心动过速，根据这些临床表现，最可能受累的冠状动脉是哪一条？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e929f7-614c-4f86-9cd0-b6417c1adbd1",
   "metadata": {},
   "source": [
    "- 问答测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c2290e9-1e9e-4fef-bde8-cc84591437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs1 = model.generate(\n",
    "    input_ids=inputs1.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response1 = tokenizer.batch_decode(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02e6c870-23e6-4f6f-a990-720dfc36887b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, so I need to figure out what cystometry would show for this 61-year-old woman with urinary incontinence during activities like coughing or sneezing. Let me start by recalling the basics.\n",
      "\n",
      "First, her symptoms: she has involuntary urine loss during physical activities. That sounds like stress urinary incontinence (SUI). SUI is usually due to weakened pelvic floor muscles or urethral sphincter, leading to leakage when there's increased abdominal pressure, like coughing, sneezing, or exercising. The fact that she doesn't leak at night suggests it's not an overactive bladder (OAB) or something nocturnal like nocturia from another cause.\n",
      "\n",
      "Now, the Q-tip test. I remember that the Q-tip test is used to assess urethral mobility. During the test, the patient is asked to strain or cough while the angle of the urethra's movement is measured. If the angle increases beyond a certain point (like more than 30 degrees), it indicates urethral hypermobility, which is a common cause of SUI. So if her Q-tip test was positive, that supports the diagnosis of SUI due to anatomical issues.\n",
      "\n",
      "Cystometry is a test that measures bladder pressure and volume during filling and voiding. The key things cystometry looks at are bladder compliance, presence of detrusor contractions, and post-void residual urine.\n",
      "\n",
      "The question asks about residual volume and detrusor contractions. Let's break it down:\n",
      "\n",
      "Residual volume: In SUI, since the issue is about urine leaking during increased pressure, the bladder's ability to store urine might not be the problem. So during cystometry, when they measure the residual urine after voiding, it should be normal (or low). High residual would suggest underactive detrusor or obstruction, which isn't typical for SUI.\n",
      "\n",
      "Detrusor contractions: Overactive bladder is characterized by involuntary detrusor contractions (detrusor overactivity) leading to urgency and urge incontinence. But in SUI, the problem isn't the bladder muscle's activity. Instead, the detrusor should be stable without involuntary contractions during the filling phase. So cystometry would likely show no detrusor contractions during filling, which differentiates it from urge incontinence.\n",
      "\n",
      "Wait, but I should make sure there's no mixed incontinence. The question says \"no leakage at night,\" which might reduce the chance of OAB, but maybe there's a little? But the key here is that the primary symptom is stress-related. The cystometry would help confirm that there's no detrusor overactivity. So the main findings would be normal residual volume and absence of detrusor contractions during the test.\n",
      "\n",
      "Putting it all together: cystometry would show normal residual volume (since she can empty well) and no detrusor contractions (since it's not urge incontinence). So the answer should state that cystometry would reveal a normal or low residual volume and absent detrusor contractions during filling.\n",
      "</think>\n",
      "\n",
      "The patient's symptoms of involuntary urine loss during activities like coughing or sneezing, combined with a negative nighttime leakage history, strongly suggest **stress urinary incontinence (SUI)**. The Q-tip test likely demonstrates urethral hypermobility, a common cause of SUI. \n",
      "\n",
      "### Cystometry Findings:\n",
      "1. **Residual Volume**:  \n",
      "   - **Normal or low residual volume** after voiding.  \n",
      "   - Rationale: SUI is caused by anatomical weakness (e.g., pelvic floor or urethral sphincter dysfunction), not impaired bladder emptying. A normal residual volume confirms the bladder is storing and emptying urine appropriately, ruling out underactive detrusor or obstruction.\n",
      "\n",
      "2. **Detrusor Contractions**:  \n",
      "   - **No involuntary detrusor contractions during bladder filling**.  \n",
      "   - Rationale: Detrusor overactivity (involuntary contractions) is a hallmark of overactive bladder (OAB)/urge incontinence, which is unrelated to SUI. The absence of detrusor contractions on cystometry differentiates SUI from mixed incontinence and confirms the primary issue is anatomical stress-related leakage rather than bladder instability.\n",
      "\n",
      "### Conclusion:  \n",
      "Cystometry would reveal **normal residual volume** and **absence of detrusor contractions** during the filling phase, supporting a diagnosis of **pure stress urinary incontinence**. This contrasts with urge incontinence or mixed incontinence, which would show detrusor overactivity on testing.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response1[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9addf3d-8181-4fbb-808f-27f1dd65f274",
   "metadata": {},
   "source": [
    "翻译如下：\n",
    "\n",
    "\\<think>  \n",
    "好的，我正在尝试分析这个医学问题。我们一步步来分解。患者是一位61岁的女性，有在咳嗽或打喷嚏等活动中发生不自主尿液流失的病史，但她夜间没有漏尿。她正在接受妇科检查和Q-tip测试。问题是关于膀胱测量（cystometry）会显示她的残余尿量和逼尿肌收缩情况。\n",
    "\n",
    "首先，我知道在像打喷嚏或咳嗽等活动中发生不自主尿液流失通常与压力性尿失禁有关。压力性尿失禁通常发生在尿道肌肉不足以在压力增大的情况下（比如咳嗽时）防止膀胱漏尿时。\n",
    "\n",
    "接下来是Q-tip测试。根据我记得的，Q-tip是一种用于测量尿道压力曲线的尿道导管。它通常用于评估尿道功能。Q-tip测试阳性结果，即在Valsalva操作过程中尿道压力低于膀胱内压，与内源性括约肌缺陷相关，这是一种压力性尿失禁类型。\n",
    "\n",
    "由于患者有在活动中出现不自主漏尿的病史，但夜间没有漏尿，更可能是压力性尿失禁，而不是像急迫性尿失禁那样的情况，急迫性尿失禁通常伴有夜间漏尿。因此，如果Q-tip测试阳性，提示内源性括约肌缺陷。\n",
    "\n",
    "现在，谈到膀胱测量。膀胱测量是一种测试，旨在测量膀胱在充盈过程中的反应以及逼尿肌的收缩情况。它可以显示是否存在膀胱过度活动症（OAB），即引起急迫感和频尿的情况，或是否存在逼尿肌低活动性，导致尿潴留。\n",
    "\n",
    "在这种情况下，患者的主要问题是压力性尿失禁，这更与无法在压力增大时保持尿液有关。膀胱测量会查看逼尿肌的收缩情况。如果逼尿肌低活动性，它将不能强有力地收缩以排空膀胱，导致残余尿量。但如果逼尿肌过度活跃，可能会收缩过度，导致急迫感。\n",
    "\n",
    "鉴于患者有压力性尿失禁的病史和Q-tip测试阳性，提示内源性括约肌缺陷，我认为膀胱测量会显示逼尿肌的收缩是正常的。问题不在于逼尿肌收缩的能力，而是无法密封尿道以保持压力。因此，残余尿量可能是正常的，除非有明显的尿潴留，但关键发现是逼尿肌的收缩是正常的，而不是过度活跃。\n",
    "\n",
    "等等，但会不会有残余尿量？如果患者排尿后膀胱中残留一些尿液，那就是残余尿量。但如果没有尿潴留的症状，比如膀胱饱胀或排尿困难，那么这种情况的可能性较小。主要问题是在活动中发生的尿失禁，因此逼尿肌收缩是正常的，残余尿量在正常范围内，除非有其他情况。\n",
    "\n",
    "所以，综合来看，膀胱测量可能会显示逼尿肌的收缩正常，残余尿量正常。问题更多是在括约肌方面，而不是逼尿肌。  \n",
    "\\</think>\n",
    "\n",
    "根据对患者病史和Q-tip测试结果的分析，膀胱测量最可能显示逼尿肌的收缩正常，残余尿量正常。主要问题似乎是由于内源性括约肌缺陷引起的压力性尿失禁，如Q-tip测试阳性所示。这种情况通常影响尿道括约肌在压力增大时防止漏尿的能力，而不是逼尿肌的收缩能力。因此，逼尿肌的收缩并未过度活跃，残余尿量在正常范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf7ec2-fcd2-4b19-91c0-0eae77f42ee4",
   "metadata": {},
   "source": [
    "标准答案：\n",
    "\n",
    "在这种压力性尿失禁的情况下，膀胱测压检查（cystometry）最可能显示**正常的排尿后残余尿量**，因为压力性尿失禁通常不会影响膀胱排空功能。此外，由于压力性尿失禁主要与**身体用力**有关，而不是膀胱过度活动症（OAB），因此在测试过程中**不太可能观察到逼尿肌的非自主收缩**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2c20cbb-56a9-471a-94a5-b1f605bc3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs2 = model.generate(\n",
    "    input_ids=inputs2.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response2 = tokenizer.batch_decode(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "786cca4c-988b-4369-b9e5-56688f1ff643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "\n",
      "Okay, let's tackle this question. The patient has sudden chest pain radiating to the neck and left arm. They have a history of hypercholesterolemia and coronary artery disease. Elevated troponin I suggests myocardial damage, so probably a heart attack. Tachycardia could be a response to pain or the heart issue itself.\n",
      "\n",
      "First, I need to recall the coronary arteries and their territories. The left main coronary artery branches into the left anterior descending (LAD) and the left circumflex (LCX). The right coronary artery (RCA) is the other main one. \n",
      "\n",
      "Chest pain location can sometimes correlate with the artery affected. But radiation to the neck and left arm... Wait, the left arm radiation is classic for cardiac issues. The LAD supplies the anterior wall. The LCX goes around the left side, and the RCA supplies the inferior wall. \n",
      "\n",
      "Wait, the left circumflex artery (LCX) might supply the lateral wall, and sometimes the left main artery if it's a proximal block. But the RCA often supplies the inferior wall, which might radiate to the jaw or upper back, maybe left shoulder. But the left arm radiation is more commonly associated with the left anterior descending artery? Or maybe the left circumflex?\n",
      "\n",
      "Alternatively, the left anterior descending artery supplies the anterior and apex, so maybe radiation to left arm? Wait, the left circumflex might supply the lateral wall and the left arm, but I'm a bit confused here.\n",
      "\n",
      "Wait, let me think again. The left arm pain is often from the median or ulnar nerves, but in cardiac cases, it's due to referred pain. The cardiac pain fibers project to the spinal cord levels of C1-C4 for the heart, so radiation can go to the neck, jaw, left arm. \n",
      "\n",
      "The distribution of the coronary arteries:\n",
      "\n",
      "- LAD: supplies anterior wall, apex, septum. Damage here (like an occlusion) would cause pain radiating to the jaw and left arm, maybe.\n",
      "\n",
      "- Circumflex (LCX): lateral wall. If the dominant artery is LCX, but often the RCA is the dominant one in many people. Wait, the dominance of the coronary arteries matters here. If the RCA is the dominant, then LCX might not be as big. But regardless, the left arm pain... \n",
      "\n",
      "Wait, the left circumflex supplies the lateral left side of the heart. The RCA supplies the inferior wall. Inferior wall MI might radiate to the upper abdomen or back, maybe the right arm? Or maybe the left. Hmm.\n",
      "\n",
      "Alternatively, the left anterior descending artery is more likely to cause pain radiating to the left arm because it's the anterior part. \n",
      "\n",
      "Wait, maybe the left circumflex and left main artery are less common. The RCA is more about inferior. \n",
      "\n",
      "Alternatively, if the left main coronary artery is blocked, that's a big problem, but the symptoms might be more severe. But given the patient has a history of CAD, maybe a specific artery is more prone.\n",
      "\n",
      "Wait, the patient has elevated troponin, so it's a myocardial infarction. The location of the infarction would indicate the artery. \n",
      "\n",
      "Chest pain radiating to the neck and left arm is classic for an anterior or lateral wall MI, which would be from LAD or LCX. \n",
      "\n",
      "But the left arm radiation is more commonly associated with left-sided heart issues. If the RCA is blocked, the inferior wall is affected, which might radiate to the jaw (like inferior to jaw?), or left arm? Wait, some sources say that inferior wall MI can radiate to the back and left arm. \n",
      "\n",
      "Hmm, maybe I need to think about the dermatomal distribution of the cardiac pain. The heart's afferent nerves come from the spinal cord levels T1-T4. The pain can refer to the area innervated by those same spinal nerves. \n",
      "\n",
      "The Cervical nerves might not be directly involved, but T1-T4. So the left arm pain would be from T2-T4, which includes the left arm. \n",
      "\n",
      "But which coronary artery supplies the area that would cause that? \n",
      "\n",
      "The LAD supplies the anterior wall. A block in the LAD could lead to anterior MI, which might present with central chest pain radiating to the left arm and jaw. \n",
      "\n",
      "The LCX supplies the lateral wall. A lateral MI might present with pain radiating to the left arm, but maybe more to the inner left arm. \n",
      "\n",
      "The RCA supplies the inferior wall. Inferior MI might radiate to the back, left shoulder, or arm. \n",
      "\n",
      "Wait, some sources say that the left arm radiation can occur in both anterior and inferior MIs. \n",
      "\n",
      "Alternatively, the left anterior descending artery is the most common site for coronary artery disease, especially in someone with hypercholesterolemia and CAD history. \n",
      "\n",
      "Given that the patient has a history of CAD and hypercholesterolemia, the most likely artery involved would be the left anterior descending (LAD) artery, leading to an anterior wall MI, which can present with the described symptoms. \n",
      "\n",
      "Alternatively, if the RCA is involved, but the left arm radiation might still occur. But the classic presentation for LAD occlusion includes left arm pain. \n",
      "\n",
      "Wait, another angle: the left circumflex artery supplies the left lateral wall. If the circumflex is occluded, the pain might radiate to the left arm. But the LAD is more central. \n",
      "\n",
      "Hmm, but the question is asking for the most likely. Given the radiation to the neck and left arm, and considering the most common coronary artery involved in such cases, it's probably the LAD. \n",
      "\n",
      "Alternatively, the left main coronary artery occlusion would cause widespread symptoms, but that's a more catastrophic event. \n",
      "\n",
      "Wait, maybe the left circumflex? Or perhaps the\n"
     ]
    }
   ],
   "source": [
    "print(response2[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f13f64-11bc-47bb-8994-e1ccea729bb9",
   "metadata": {},
   "source": [
    "翻译如下：\n",
    "\n",
    "\\<think>  \n",
    "好的，我有一位患者，突然出现胸痛，并放射到颈部和左臂。这让我想到了心脏病发作，因为这些症状很经典——心绞痛或心肌梗死。左臂痛、颈部痛，有时还会伴随下颌或背部的疼痛，这些都可能与冠状动脉问题相关。\n",
    "\n",
    "从病史来看，患者有**高胆固醇血症**（即高胆固醇）和**冠状动脉疾病**，这两个因素都是**动脉粥样硬化**的风险因素，可能导致冠状动脉发生堵塞。**肌钙蛋白I升高**是一个很大的线索，因为肌钙蛋白是心肌受损时释放的心脏酶，通常表明发生了心肌梗死。另外，患者还出现了**心动过速**，即心跳比平常快。在心肌梗死时，心脏可能会加速跳动，以试图通过增加心脏输出量来补偿被阻塞的冠状动脉。\n",
    "\n",
    "考虑到冠状动脉，**左主冠状动脉**（LMCA）为整个左侧心脏提供血液，包括左心室，而左心室是一个关键的泵血肌肉。如果这里发生堵塞，可能导致更严重的心肌梗死，因为左心室至关重要。**右冠状动脉**为右心室和左心室下壁提供血液，这里的堵塞也是可能的，但**左主冠状动脉**通常与上述症状更相关，尤其是当肌钙蛋白升高时。\n",
    "\n",
    "所以，将所有因素综合考虑，最可能受累的冠状动脉是**左主冠状动脉**（LMCA）。患者的病史、肌钙蛋白升高以及典型的胸痛放射症状都指向了这一动脉作为罪魁祸首。\n",
    "\n",
    "\\</think>\n",
    "\n",
    "最可能受累的冠状动脉是**左主冠状动脉（LMCA）**。\n",
    "\n",
    "**解释：**\n",
    "- **症状：** 患者突发胸痛并放射至颈部和左臂，以及肌钙蛋白升高，提示急性冠状动脉综合症，可能是心肌梗死。\n",
    "- **病史：** 高胆固醇血症和冠状动脉疾病病史是动脉粥样硬化的风险因素，可能导致冠状动脉堵塞。\n",
    "- **心动过速：** 心率增加可能是心脏为补偿心肌血流减少而产生的反应。\n",
    "- **冠状动脉考虑：** 左主冠状动脉供应左心室，这个肌肉对心脏功能至关重要。与右冠状动脉相比，左主冠状动脉的堵塞会导致更严重且危及生命的心肌梗死，右冠状动脉通常供应的是不那么关键的区域。\n",
    "\n",
    "因此，症状、肌钙蛋白升高以及患者的病史强烈指向**左主冠状动脉**（LMCA）作为最可能的罪魁祸首。<｜end▁of▁sentence｜>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acf498-88f5-47b7-831e-2dcbcb96e589",
   "metadata": {},
   "source": [
    "标准答案：\n",
    "\n",
    "根据患者表现出的突然胸痛并放射至颈部和左臂，结合其有高胆固醇血症和冠状动脉疾病的病史，肌钙蛋白升高和心动过速，临床症状强烈提示左前降支（LAD）动脉受累。该动脉通常是引发此类症状的罪魁祸首，因为它供应了心脏的大部分区域。放射性疼痛和肌钙蛋白升高的组合表明心肌受损，这使得LAD成为最可能的致病动脉。然而，在没有进一步的诊断检查（如心电图）的情况下，最终的确诊仍需等待确认。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7cdfee-2b25-4fdc-9ecd-ac5ea4a30e41",
   "metadata": {},
   "source": [
    "能够看出，在原始状态下，模型能够进行推理并给出回复，但实际上第一个回答过程并不符合医学规范，而第二个问题则直接回答错误。由此可见，在初始状态下，模型对于medical-o1-reasoning-SFT数据集问答效果并不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2dc95-c4c8-4a03-acb9-b8c5bce92e9d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来尝试进行微调，并测试微调后模型问答效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e5d96-c090-4d06-b133-c36b789e41b8",
   "metadata": {},
   "source": [
    "### 三、最小可行性实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6842985-e49e-44f3-9fa5-2e2ba3ec4a3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们尝试进行模型微调，对于当前数据集而言，我们可以带入原始数据集的部分数据进行微调，也可以带入全部数据并遍历多次进行微调。对于大多数的微调实验，我们都可以从最小可行性实验入手进行微调，也就是先尝试带入少量数据进行微调，并观测微调效果。若微调可以顺利执行，并能够获得微调效果，再考虑带入更多的数据进行更大规模微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672971b-0d59-4c5a-9461-fa7e4c8e1cb4",
   "metadata": {},
   "source": [
    "#### 1.数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dc132-747d-40ec-9269-f315678b29e0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们直接从huggingface上下载medical-o1-reasoning-SFT数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b548-4e41-406b-8d52-110509156eb9",
   "metadata": {},
   "source": [
    "- 设置代理环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ac067-8d97-4999-9a3e-1ef305751f39",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于huggingface网络受限，下载数据集前需要先进行网络环境设置。若是AutoDL服务器，则可以按照如下方式开启学术加速，从而顺利连接huggingface并进行数据集下载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "281d6b32-4b56-4184-80da-202d7dd589af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef1f84-74f4-4779-9326-a11502aa6ae1",
   "metadata": {},
   "source": [
    "- 下载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d4f6f-38ce-438e-be64-951f4b907fd7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来使用datasets进行数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5924a688-b61e-4d6d-bc2e-fa0b8c013ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4635bd-5d5d-406b-900d-eccbfaf261a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90685ac-6510-4fd4-9c2d-def17e0dc6d5",
   "metadata": {},
   "source": [
    "再次确认提示词模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acef18a3-d669-4acf-8da7-4da22b3aaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53db946-57c9-40c0-aded-1ac2462f12ec",
   "metadata": {},
   "source": [
    "然后提取并设置文本生成结束的标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c75cc9-61f4-4ff3-9124-34ff78456a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117c824-4486-4b17-bbd0-cb3ff9a34f4d",
   "metadata": {},
   "source": [
    "然后定义函数，用于对medical-o1-reasoning-SFT数据集进行修改，Complex_CoT列和Response列进行拼接，并加上文本结束标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dee28cc-1fe8-4c79-860b-c7792b269530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b85065-f6a9-4bff-ae58-cdd940bb2cbf",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206180316919.png\" alt=\"image-20250206180316919\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4456066-89d8-4ad0-9a2c-edf72eca77f9",
   "metadata": {},
   "source": [
    "在最小可行性实验中，我们可以只下载500条数据进行微调即可看出效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c18961a1-0db6-404f-81ad-182b2b62a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85106a25-688e-447f-b411-11cc1b0206df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?',\n",
       " 'Complex_CoT': \"Okay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\",\n",
       " 'Response': 'Cystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca9256-067a-4f7b-a339-e0473bc4a81e",
   "metadata": {},
   "source": [
    "然后进行结构化处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82c249c5-ec25-448d-8045-46042d9ad963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc62f8-96f2-4905-b0e6-c28a60954ce7",
   "metadata": {},
   "source": [
    "将数据集整理为如下形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f34f952-656e-44d6-a778-8a70690d4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<|im_end|>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478885f-5a4b-431c-b705-6faea5bcf087",
   "metadata": {},
   "source": [
    "- 数据集保存地址"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ca9f9-34ed-45d8-be58-b4ed829dbcbf",
   "metadata": {},
   "source": [
    "默认情况下数据集保存在主目录下.cache文件夹中，数据文件格式如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a301ee-a527-4d47-8074-6bd61f5a4bad",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311161225502.png\" alt=\"image-20250311161225502\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a4418-2f7b-4b3e-9efa-efa2a5fd6dd7",
   "metadata": {},
   "source": [
    "#### 2.开启微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7956b-039b-483f-9b50-15caf056dfb9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后即可把模型设置为微调模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028f9bf9-c089-4546-af8e-9eb56ca31b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.9 patched 64 layers with 64 QKV layers, 64 O layers and 64 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d5b06-50b4-4d78-a902-e2df87221f3f",
   "metadata": {},
   "source": [
    "然后导入相关的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d135473-1c60-4555-9a14-8abd5e5cabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e168c5-df66-4a26-b65d-ad5251db7e76",
   "metadata": {},
   "source": [
    "创建有监督微调对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e1622f1-a0d9-495e-8162-8623027d4a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1e723b8f2c48ea96e9666063e15323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing to [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836f8b5-b608-4af2-be5c-c1a4d4de74c5",
   "metadata": {},
   "source": [
    "这段代码主要是用 **`SFTTrainer`** 进行 **监督微调（Supervised Fine-Tuning, SFT）**，适用于 `transformers` 和 `Unsloth` 生态中的模型微调：\n",
    "**1. 导入相关库**\n",
    "- **`SFTTrainer`**（来自 `trl` 库）：  \n",
    "  - `trl`（Transformer Reinforcement Learning）是 Hugging Face 旗下的 `trl` 库，提供 **监督微调（SFT）** 和 **强化学习（RLHF）** 相关的功能。\n",
    "  - `SFTTrainer` 主要用于 **有监督微调（Supervised Fine-Tuning）**，适用于 `LoRA` 等低秩适配微调方式。\n",
    "\n",
    "- **`TrainingArguments`**（来自 `transformers` 库）：  \n",
    "  - 这个类用于定义 **训练超参数**，比如批量大小、学习率、优化器、训练步数等。\n",
    "\n",
    "- **`is_bfloat16_supported()`**（来自 `unsloth`）：  \n",
    "  - 这个函数检查 **当前 GPU 是否支持 `bfloat16`（BF16）**，如果支持，则返回 `True`，否则返回 `False`。\n",
    "  - `bfloat16` 是一种更高效的数值格式，在 **新款 NVIDIA A100/H100** 等 GPU 上表现更优。\n",
    "\n",
    "**2. 初始化 `SFTTrainer` 进行模型微调**\n",
    "\n",
    "##### **参数解析**\n",
    "##### **① `SFTTrainer` 部分**\n",
    "| 参数 | 作用 |\n",
    "|------|------|\n",
    "| `model=model` | 指定需要进行微调的 **预训练模型** |\n",
    "| `tokenizer=tokenizer` | 指定 **分词器**，用于处理文本数据 |\n",
    "| `train_dataset=dataset` | 传入 **训练数据集** |\n",
    "| `dataset_text_field=\"text\"` | 指定数据集中哪一列包含 **训练文本**（在 `formatting_prompts_func` 里处理） |\n",
    "| `max_seq_length=max_seq_length` | **最大序列长度**，控制输入文本的最大 Token 数量 |\n",
    "| `dataset_num_proc=2` | **数据加载的并行进程数**，提高数据预处理效率 |\n",
    "\n",
    "##### **② `TrainingArguments` 部分**\n",
    "| 参数 | 作用 |\n",
    "|------|------|\n",
    "| `per_device_train_batch_size=2` | 每个 **GPU/设备** 的训练批量大小（较小值适合大模型） |\n",
    "| `gradient_accumulation_steps=4` | **梯度累积步数**（相当于 `batch_size=2 × 4 = 8`） |\n",
    "| `warmup_steps=5` | **预热步数**（初始阶段学习率较低，然后逐步升高） |\n",
    "| `max_steps=60` | **最大训练步数**（控制训练的总步数，此处总共约消耗60*8=480条数据） |\n",
    "| `learning_rate=2e-4` | **学习率**（`2e-4` = 0.0002，控制权重更新幅度） |\n",
    "| `fp16=not is_bfloat16_supported()` | 如果 **GPU 不支持 `bfloat16`，则使用 `fp16`（16位浮点数）** |\n",
    "| `bf16=is_bfloat16_supported()` | 如果 **GPU 支持 `bfloat16`，则启用 `bfloat16`（训练更稳定）** |\n",
    "| `logging_steps=10` | **每 10 步记录一次训练日志** |\n",
    "| `optim=\"adamw_8bit\"` | **使用 `adamw_8bit`（8-bit AdamW优化器）减少显存占用** |\n",
    "| `weight_decay=0.01` | **权重衰减（L2 正则化）**，防止过拟合 |\n",
    "| `lr_scheduler_type=\"linear\"` | **学习率调度策略**（线性衰减） |\n",
    "| `seed=3407` | **随机种子**（保证实验结果可复现） |\n",
    "| `output_dir=\"outputs\"` | **训练结果的输出目录** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cc26d-f6c9-4979-bb12-9c07849bcf06",
   "metadata": {},
   "source": [
    "然后设置wandb（可选）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "536a41b7-50bf-4f0f-8240-777b3c80f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5caee957-cc73-46b8-845f-e6119d94f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2323365771\u001b[0m (\u001b[33m2323365771-ff\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"YOUR_WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3d7783c-e1a9-42e0-b441-7673ea0d6d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.8s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/wandb/run-20250311_161713-8wqmaye1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset/runs/8wqmaye1' target=\"_blank\">wise-flower-1</a></strong> to <a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset' target=\"_blank\">https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset/runs/8wqmaye1' target=\"_blank\">https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset/runs/8wqmaye1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Fine-tune-QwQ-32B-4bit on Medical COT Dataset', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea7966-4da4-463e-b4c4-cf628b74f3d2",
   "metadata": {},
   "source": [
    "然后开始微调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd782462-3d29-490c-9d63-4cb43a240883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 134,217,728/18,388,423,680 (0.73% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:32, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.179900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1278f09-4a99-4833-900a-659cd3e002eb",
   "metadata": {},
   "source": [
    "此时wandb中显示内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107a62e-eed6-49ad-8c65-7b51b12819ba",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311162939444.png\" alt=\"image-20250311162939444\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05245a7d-6239-436d-847d-5d3188e1ec52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.2699724833170574, metrics={'train_runtime': 665.1344, 'train_samples_per_second': 1.443, 'train_steps_per_second': 0.09, 'total_flos': 1.740613665982464e+17, 'train_loss': 1.2699724833170574})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa75b2-0345-4c18-8eda-ebf1241b269b",
   "metadata": {},
   "source": [
    "注意，unsloth在微调结束后，会自动更新模型权重（在缓存中），因此无需手动合并模型权重即可直接调用微调后的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efaf4cf4-ef13-4e3f-bc7f-d08fd36ffc4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (1-3): 3 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (4-5): 2 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (6-42): 37 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (43): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (44-59): 16 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (60): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (61): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (62): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (63): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "64bca6ee-fc4c-4ca0-ab9d-d099f628f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bbb2f57-8e83-484f-b8b6-e34d9d9a529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think this through. We've got this woman, 61 years old, and she's been dealing with involuntary urine leakage for a while, especially during activities like coughing or sneezing. That's classic stress urinary incontinence, right? It usually happens when there's some pressure on the bladder from things like laughing or lifting something heavy.\n",
      "\n",
      "Now, she's been checked out by a gynecologist and they did a Q-tip test. From what I remember, the Q-tip test is all about figuring out how well the urethra is supported. If the angle is too steep, that could mean the urethra isn't supported properly, which is a sign of stress incontinence.\n",
      "\n",
      "So, if she's got stress urinary incontinence, what does that say about her bladder's behavior? Stress incontinence is mainly about the urethral support and not about the bladder muscle itself. That means the bladder muscle, or the detrusor, isn't really misbehaving here. It should be calm and not causing any issues.\n",
      "\n",
      "Now, let's talk about residual volume. Residual volume is the urine left in the bladder after someone goes. In stress incontinence, since it's not a problem with the bladder muscle, you'd expect the bladder to empty normally. So, there shouldn't be much urine left behind, which means the residual volume should be low.\n",
      "\n",
      "And what about the detrusor contractions? Since the issue is with the urethral support and not the bladder muscle, the detrusor should be relaxed and not causing any unwanted contractions. So, during a cystometry test, they should see that the detrusor is pretty chill.\n",
      "\n",
      "Putting it all together, if she's got stress urinary incontinence, the cystometry should show a low residual volume and no detrusor contractions. That makes sense because it's all about the urethra's support, not the bladder's activity.\n",
      "</think>\n",
      "Based on the symptoms and findings consistent with stress urinary incontinence, cystometry would most likely reveal a low residual volume and no detrusor contractions. Stress urinary incontinence typically involves issues with urethral support rather than bladder muscle activity, so the detrusor muscle should remain relaxed, and the bladder should empty normally, leaving little residual urine.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46783905-93f7-4886-9920-2ca46d836593",
   "metadata": {},
   "source": [
    "测试第二个问题问答效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b850a79-eba4-44fc-98d8-1488a85742a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1834b90-c3e2-4a0b-b967-0293726a78a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's think about this. The patient is having chest pain that's suddenly come on, and it's radiating to the neck and left arm. That sounds like classic angina, maybe even a heart attack. The fact that it's radiating is a big clue—it often means it's coming from the heart, not something else.\n",
      "\n",
      "Now, the patient has a history of hypercholesterolemia and coronary artery disease. So, we know they've got some pre-existing issues with their heart. They're also showing elevated troponin I levels, which is a sign of heart muscle damage. That really makes me think they're having a heart attack, or myocardial infarction.\n",
      "\n",
      "And then there's the tachycardia. That's a fast heart rate, which could be a reaction to the stress of a heart attack or just the pain itself.\n",
      "\n",
      "Now, let's think about which coronary artery is likely involved. The left anterior descending artery, or LAD, is really important because it supplies a big chunk of the heart, including the anterior wall and the septum. If it gets blocked, it can cause a massive heart attack, and the pain would definitely radiate to the neck and left arm.\n",
      "\n",
      "So, putting it all together, the sudden chest pain, the history of coronary artery disease, the elevated troponin, and the radiation pattern all point to the LAD being the culprit. It's the most logical conclusion given all these signs.\n",
      "</think>\n",
      "Based on the patient's symptoms and medical history, the most likely coronary artery involved in this scenario is the left anterior descending artery (LAD). The LAD is a major coronary artery that supplies blood to the anterior wall of the heart and the interventricular septum. A blockage in the LAD can lead to significant heart muscle damage, which aligns with the elevated troponin I levels indicating myocardial infarction. The sudden-onset chest pain radiating to the neck and left arm, along with the history of coronary artery disease and hypercholesterolemia, further supports this conclusion.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb726b1-1e9a-45e3-a197-992e67e1e5b3",
   "metadata": {},
   "source": [
    "> 此时模型认为“左主冠状动脉最可能是该患者症状的罪魁祸首”，但实际上应该是“左前降支（LAD）动脉受累”导致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7c4e0-d1cd-4fc9-8dbd-c76ed581f2b3",
   "metadata": {},
   "source": [
    "能够发现，第一个问题回答更加规范，并且回答正确。但第二个问题仍然回答错误。由此可以考虑继续进行大规模微调。不过在此之前，我们可以将现在小规模微调的模型进行本地保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83871526-b408-4090-8ea5-73ed0a7daba7",
   "metadata": {},
   "source": [
    "#### 3.模型合并"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b846d-ebe6-4b9a-a5ff-e5971258e3fc",
   "metadata": {},
   "source": [
    "此时本地保存的模型权重在`outputs`文件夹中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378c031-80f1-4eac-9775-7557188981ab",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311163146066.png\" alt=\"image-20250311163146066\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e9f53-0ee6-4830-86b9-4ba2f04c5169",
   "metadata": {},
   "source": [
    "然后可使用如下代码进行模型权重合并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74072db4-3de1-480c-845a-cdcde2447483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 657.42 out of 1007.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████████████▎                                        | 36/64 [00:01<00:01, 25.68it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:59<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"QwQ-Medical-COT-Tiny\", tokenizer, save_method = \"merged_4bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3acbd1d4-1715-40c8-b40b-6df5491f54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\"QwQ-Medical-COT-Tiny\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7de938-a6fe-4c43-bae3-f7c6187e8e69",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311165148556.png\" alt=\"image-20250311165148556\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1e223-28fd-45c4-b71c-86f6be768a23",
   "metadata": {},
   "source": [
    "此外，也可以使用如下代码将其保存为GGUF格式，方便使用ollama进行推理。本部分导出与合并需要较长时间（约20分钟左右），请耐心等待。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf42284a-0bd1-4eb1-abc3-d89cabe472dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 744.1 out of 1007.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:43<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at QwQ-Medical-COT-Tiny-GGUF into bf16 GGUF format.\n",
      "The output location will be /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: QwQ-Medical-COT-Tiny-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00007-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00008-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00009-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00010-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.48.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.48.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.48.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.48.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.48.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00011-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.48.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.48.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.48.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.48.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.49.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.49.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.49.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.50.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.50.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.50.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.50.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.51.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.51.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.51.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.51.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.52.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.52.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.52.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.52.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.53.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.53.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.53.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.53.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.53.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00012-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.53.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.53.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.53.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.53.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.54.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.54.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.54.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.55.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.55.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.55.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.55.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.56.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.56.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.56.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.56.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.57.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.57.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.57.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.57.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.58.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.58.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.58.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.58.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.58.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00013-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.58.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.58.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.58.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.58.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.59.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.59.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.59.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.60.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.60.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.60.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.60.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.61.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.61.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.61.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.61.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.62.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.62.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.62.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.62.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.63.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.63.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.63.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.63.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.63.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00014-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.63.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.63.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.63.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.63.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 27648\n",
      "INFO:hf-to-gguf:gguf: head count = 40\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- '' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "  {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n",
      "        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n<think>\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf: n_tensors = 771, total_size = 65.5G\n",
      "Writing: 100%|██████████| 65.5G/65.5G [08:58<00:00, 122Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 4743 (d07c6213)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf' to '/root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 352 threads\n",
      "llama_model_loader: loaded meta data with 25 key-value pairs and 771 tensors from /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = QwQ 32B Unsloth Bnb 4bit\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = unsloth-bnb-4bit\n",
      "llama_model_loader: - kv   4:                           general.basename str              = QwQ\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 64\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  321 tensors\n",
      "llama_model_loader: - type bf16:  450 tensors\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA H800 PCIe, compute capability 9.0, VMM: yes\n",
      "  Device 1: NVIDIA H800 PCIe, compute capability 9.0, VMM: yes\n",
      "[   1/ 771]                        output.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1485.00 MiB ->   609.08 MiB\n",
      "[   2/ 771]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   3/ 771]                    token_embd.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1485.00 MiB ->   417.66 MiB\n",
      "[   4/ 771]                    blk.0.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[   5/ 771]                  blk.0.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[   6/ 771]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   7/ 771]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[   8/ 771]                    blk.0.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   9/ 771]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  10/ 771]                    blk.0.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  11/ 771]                  blk.0.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  12/ 771]                blk.0.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  13/ 771]                blk.0.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  14/ 771]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  15/ 771]                  blk.0.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  16/ 771]                    blk.1.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  17/ 771]                  blk.1.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  18/ 771]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  19/ 771]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  20/ 771]                    blk.1.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  21/ 771]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  22/ 771]                    blk.1.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  23/ 771]                  blk.1.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  24/ 771]                blk.1.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  25/ 771]                blk.1.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  26/ 771]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  27/ 771]                  blk.1.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  28/ 771]                    blk.2.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  29/ 771]                  blk.2.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  30/ 771]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  31/ 771]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  32/ 771]                    blk.2.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  33/ 771]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  34/ 771]                    blk.2.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  35/ 771]                  blk.2.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  36/ 771]                blk.2.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  37/ 771]                blk.2.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  38/ 771]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  39/ 771]                  blk.2.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  40/ 771]                    blk.3.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  41/ 771]                  blk.3.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  42/ 771]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  43/ 771]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  44/ 771]                    blk.3.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  45/ 771]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  46/ 771]                    blk.3.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  47/ 771]                  blk.3.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  48/ 771]                blk.3.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  49/ 771]                blk.3.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  50/ 771]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  51/ 771]                  blk.3.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  52/ 771]                    blk.4.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  53/ 771]                  blk.4.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  54/ 771]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  55/ 771]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  56/ 771]                    blk.4.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  57/ 771]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  58/ 771]                    blk.4.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  59/ 771]                  blk.4.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  60/ 771]                blk.4.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  61/ 771]                blk.4.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  62/ 771]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  63/ 771]                  blk.4.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  64/ 771]                    blk.5.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  65/ 771]                  blk.5.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  66/ 771]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  67/ 771]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  68/ 771]                    blk.5.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  69/ 771]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  70/ 771]                    blk.5.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  71/ 771]                  blk.5.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  72/ 771]                blk.5.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  73/ 771]                blk.5.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  74/ 771]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  75/ 771]                  blk.5.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  76/ 771]                    blk.6.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  77/ 771]                  blk.6.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  78/ 771]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  79/ 771]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  80/ 771]                    blk.6.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  81/ 771]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  82/ 771]                    blk.6.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  83/ 771]                  blk.6.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  84/ 771]                blk.6.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  85/ 771]                blk.6.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  86/ 771]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  87/ 771]                  blk.6.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  88/ 771]                    blk.7.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  89/ 771]                  blk.7.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  90/ 771]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  91/ 771]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  92/ 771]                    blk.7.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  93/ 771]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  94/ 771]                    blk.7.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  95/ 771]                  blk.7.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  96/ 771]                blk.7.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  97/ 771]                blk.7.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  98/ 771]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  99/ 771]                  blk.7.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 100/ 771]                    blk.8.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 101/ 771]                  blk.8.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 102/ 771]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 103/ 771]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 104/ 771]                    blk.8.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 105/ 771]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 106/ 771]                    blk.8.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 107/ 771]                  blk.8.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 108/ 771]                blk.8.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 109/ 771]                blk.8.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 110/ 771]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 111/ 771]                  blk.8.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 112/ 771]                    blk.9.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 113/ 771]                  blk.9.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 114/ 771]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 115/ 771]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 116/ 771]                    blk.9.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 117/ 771]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 118/ 771]                    blk.9.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 119/ 771]                  blk.9.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 120/ 771]                blk.9.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 121/ 771]                blk.9.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 122/ 771]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 123/ 771]                  blk.9.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 124/ 771]                   blk.10.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 125/ 771]                 blk.10.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 126/ 771]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 127/ 771]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 128/ 771]                   blk.10.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 129/ 771]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 130/ 771]                   blk.10.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 131/ 771]                 blk.10.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 132/ 771]               blk.10.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 133/ 771]               blk.10.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 134/ 771]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 135/ 771]                 blk.10.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 136/ 771]                   blk.11.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 137/ 771]                 blk.11.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 138/ 771]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 139/ 771]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 140/ 771]                   blk.11.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 141/ 771]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 142/ 771]                   blk.11.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 143/ 771]                 blk.11.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 144/ 771]               blk.11.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 145/ 771]               blk.11.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 146/ 771]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 147/ 771]                 blk.11.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 148/ 771]                   blk.12.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 149/ 771]                 blk.12.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 150/ 771]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 151/ 771]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 152/ 771]                   blk.12.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 153/ 771]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 154/ 771]                   blk.12.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 155/ 771]                 blk.12.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 156/ 771]               blk.12.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 157/ 771]               blk.12.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 158/ 771]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 159/ 771]                 blk.12.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 160/ 771]                   blk.13.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 161/ 771]                 blk.13.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 162/ 771]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 163/ 771]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 164/ 771]                   blk.13.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 165/ 771]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 166/ 771]                   blk.13.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 167/ 771]                 blk.13.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 168/ 771]               blk.13.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 169/ 771]               blk.13.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 170/ 771]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 171/ 771]                 blk.13.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 172/ 771]                   blk.14.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 173/ 771]                 blk.14.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 174/ 771]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 175/ 771]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 176/ 771]                   blk.14.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 177/ 771]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 178/ 771]                   blk.14.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 179/ 771]                 blk.14.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 180/ 771]               blk.14.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 181/ 771]               blk.14.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 182/ 771]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 183/ 771]                 blk.14.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 184/ 771]                   blk.15.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 185/ 771]                 blk.15.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 186/ 771]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 187/ 771]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 188/ 771]                   blk.15.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 189/ 771]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 190/ 771]                   blk.15.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 191/ 771]                 blk.15.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 192/ 771]               blk.15.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 193/ 771]               blk.15.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 194/ 771]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 195/ 771]                 blk.15.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 196/ 771]                   blk.16.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 197/ 771]                 blk.16.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 198/ 771]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 199/ 771]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 200/ 771]                   blk.16.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 201/ 771]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 202/ 771]                   blk.16.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 203/ 771]                 blk.16.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 204/ 771]               blk.16.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 205/ 771]               blk.16.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 206/ 771]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 207/ 771]                 blk.16.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 208/ 771]                   blk.17.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 209/ 771]                 blk.17.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 210/ 771]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 211/ 771]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 212/ 771]                   blk.17.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 213/ 771]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 214/ 771]                   blk.17.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 215/ 771]                 blk.17.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 216/ 771]               blk.17.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 217/ 771]               blk.17.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 218/ 771]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 219/ 771]                 blk.17.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 220/ 771]                   blk.18.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 221/ 771]                 blk.18.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 222/ 771]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 223/ 771]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 224/ 771]                   blk.18.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 225/ 771]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 226/ 771]                   blk.18.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 227/ 771]                 blk.18.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 228/ 771]               blk.18.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 229/ 771]               blk.18.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 230/ 771]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 231/ 771]                 blk.18.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 232/ 771]                   blk.19.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 233/ 771]                 blk.19.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 234/ 771]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 235/ 771]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 236/ 771]                   blk.19.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 237/ 771]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 238/ 771]                   blk.19.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 239/ 771]                 blk.19.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 240/ 771]               blk.19.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 241/ 771]               blk.19.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 242/ 771]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 243/ 771]                 blk.19.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 244/ 771]                   blk.20.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 245/ 771]                 blk.20.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 246/ 771]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 247/ 771]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 248/ 771]                   blk.20.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 249/ 771]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 250/ 771]                   blk.20.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 251/ 771]                 blk.20.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 252/ 771]               blk.20.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 253/ 771]               blk.20.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 254/ 771]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 255/ 771]                 blk.20.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 256/ 771]                   blk.21.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 257/ 771]                 blk.21.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 258/ 771]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 259/ 771]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 260/ 771]                   blk.21.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 261/ 771]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 262/ 771]                   blk.21.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 263/ 771]                 blk.21.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 264/ 771]               blk.21.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 265/ 771]               blk.21.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 266/ 771]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 267/ 771]                 blk.21.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 268/ 771]                   blk.22.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 269/ 771]                 blk.22.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 270/ 771]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 271/ 771]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 272/ 771]                   blk.22.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 273/ 771]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 274/ 771]                   blk.22.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 275/ 771]                 blk.22.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 276/ 771]               blk.22.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 277/ 771]               blk.22.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 278/ 771]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 279/ 771]                 blk.22.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 280/ 771]                   blk.23.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 281/ 771]                 blk.23.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 282/ 771]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 283/ 771]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 284/ 771]                   blk.23.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 285/ 771]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 286/ 771]                   blk.23.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 287/ 771]                 blk.23.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 288/ 771]               blk.23.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 289/ 771]               blk.23.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 290/ 771]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 291/ 771]                 blk.23.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 292/ 771]                   blk.24.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 293/ 771]                 blk.24.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 294/ 771]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 295/ 771]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 296/ 771]                   blk.24.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 297/ 771]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 298/ 771]                   blk.24.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 299/ 771]                 blk.24.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 300/ 771]               blk.24.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 301/ 771]               blk.24.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 302/ 771]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 303/ 771]                 blk.24.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 304/ 771]                   blk.25.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 305/ 771]                 blk.25.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 306/ 771]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 307/ 771]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 308/ 771]                   blk.25.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 309/ 771]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 310/ 771]                   blk.25.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 311/ 771]                 blk.25.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 312/ 771]               blk.25.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 313/ 771]               blk.25.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 314/ 771]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 315/ 771]                 blk.25.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 316/ 771]                   blk.26.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 317/ 771]                 blk.26.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 318/ 771]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 319/ 771]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 320/ 771]                   blk.26.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 321/ 771]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 322/ 771]                   blk.26.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 323/ 771]                 blk.26.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 324/ 771]               blk.26.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 325/ 771]               blk.26.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 326/ 771]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 327/ 771]                 blk.26.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 328/ 771]                   blk.27.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 329/ 771]                 blk.27.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 330/ 771]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 331/ 771]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 332/ 771]                   blk.27.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 333/ 771]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 334/ 771]                   blk.27.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 335/ 771]                 blk.27.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 336/ 771]               blk.27.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 337/ 771]               blk.27.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 338/ 771]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 339/ 771]                 blk.27.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 340/ 771]                   blk.28.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 341/ 771]                 blk.28.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 342/ 771]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 343/ 771]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 344/ 771]                   blk.28.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 345/ 771]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 346/ 771]                   blk.28.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 347/ 771]                 blk.28.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 348/ 771]               blk.28.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 349/ 771]               blk.28.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 350/ 771]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 351/ 771]                 blk.28.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 352/ 771]                   blk.29.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 353/ 771]                 blk.29.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 354/ 771]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 355/ 771]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 356/ 771]                   blk.29.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 357/ 771]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 358/ 771]                   blk.29.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 359/ 771]                 blk.29.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 360/ 771]               blk.29.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 361/ 771]               blk.29.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 362/ 771]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 363/ 771]                 blk.29.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 364/ 771]                   blk.30.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 365/ 771]                 blk.30.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 366/ 771]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 367/ 771]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 368/ 771]                   blk.30.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 369/ 771]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 370/ 771]                   blk.30.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 371/ 771]                 blk.30.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 372/ 771]               blk.30.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 373/ 771]               blk.30.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 374/ 771]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 375/ 771]                 blk.30.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 376/ 771]                   blk.31.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 377/ 771]                 blk.31.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 378/ 771]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 379/ 771]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 380/ 771]                   blk.31.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 381/ 771]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 382/ 771]                   blk.31.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 383/ 771]                 blk.31.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 384/ 771]               blk.31.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 385/ 771]               blk.31.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 386/ 771]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 387/ 771]                 blk.31.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 388/ 771]                   blk.32.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 389/ 771]                 blk.32.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 390/ 771]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 391/ 771]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 392/ 771]                   blk.32.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 393/ 771]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 394/ 771]                   blk.32.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 395/ 771]                 blk.32.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 396/ 771]               blk.32.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 397/ 771]               blk.32.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 398/ 771]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 399/ 771]                 blk.32.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 400/ 771]                   blk.33.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 401/ 771]                 blk.33.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 402/ 771]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 403/ 771]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 404/ 771]                   blk.33.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 405/ 771]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 406/ 771]                   blk.33.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 407/ 771]                 blk.33.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 408/ 771]               blk.33.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 409/ 771]               blk.33.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 410/ 771]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 411/ 771]                 blk.33.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 412/ 771]                   blk.34.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 413/ 771]                 blk.34.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 414/ 771]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 415/ 771]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 416/ 771]                   blk.34.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 417/ 771]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 418/ 771]                   blk.34.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 419/ 771]                 blk.34.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 420/ 771]               blk.34.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 421/ 771]               blk.34.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 422/ 771]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 423/ 771]                 blk.34.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 424/ 771]                   blk.35.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 425/ 771]                 blk.35.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 426/ 771]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 427/ 771]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 428/ 771]                   blk.35.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 429/ 771]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 430/ 771]                   blk.35.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 431/ 771]                 blk.35.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 432/ 771]               blk.35.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 433/ 771]               blk.35.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 434/ 771]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 435/ 771]                 blk.35.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 436/ 771]                   blk.36.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 437/ 771]                 blk.36.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 438/ 771]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 439/ 771]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 440/ 771]                   blk.36.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 441/ 771]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 442/ 771]                   blk.36.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 443/ 771]                 blk.36.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 444/ 771]               blk.36.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 445/ 771]               blk.36.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 446/ 771]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 447/ 771]                 blk.36.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 448/ 771]                   blk.37.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 449/ 771]                 blk.37.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 450/ 771]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 451/ 771]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 452/ 771]                   blk.37.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 453/ 771]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 454/ 771]                   blk.37.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 455/ 771]                 blk.37.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 456/ 771]               blk.37.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 457/ 771]               blk.37.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 458/ 771]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 459/ 771]                 blk.37.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 460/ 771]                   blk.38.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 461/ 771]                 blk.38.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 462/ 771]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 463/ 771]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 464/ 771]                   blk.38.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 465/ 771]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 466/ 771]                   blk.38.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 467/ 771]                 blk.38.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 468/ 771]               blk.38.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 469/ 771]               blk.38.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 470/ 771]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 471/ 771]                 blk.38.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 472/ 771]                   blk.39.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 473/ 771]                 blk.39.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 474/ 771]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 475/ 771]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 476/ 771]                   blk.39.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 477/ 771]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 478/ 771]                   blk.39.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 479/ 771]                 blk.39.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 480/ 771]               blk.39.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 481/ 771]               blk.39.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 482/ 771]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 483/ 771]                 blk.39.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 484/ 771]                   blk.40.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 485/ 771]                 blk.40.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 486/ 771]              blk.40.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 487/ 771]            blk.40.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 488/ 771]                   blk.40.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 489/ 771]                 blk.40.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 490/ 771]                   blk.40.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 491/ 771]                 blk.40.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 492/ 771]               blk.40.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 493/ 771]               blk.40.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 494/ 771]               blk.40.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 495/ 771]                 blk.40.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 496/ 771]                   blk.41.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 497/ 771]                 blk.41.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 498/ 771]              blk.41.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 499/ 771]            blk.41.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 500/ 771]                   blk.41.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 501/ 771]                 blk.41.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 502/ 771]                   blk.41.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 503/ 771]                 blk.41.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 504/ 771]               blk.41.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 505/ 771]               blk.41.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 506/ 771]               blk.41.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 507/ 771]                 blk.41.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 508/ 771]                   blk.42.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 509/ 771]                 blk.42.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 510/ 771]              blk.42.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 511/ 771]            blk.42.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 512/ 771]                   blk.42.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 513/ 771]                 blk.42.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 514/ 771]                   blk.42.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 515/ 771]                 blk.42.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 516/ 771]               blk.42.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 517/ 771]               blk.42.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 518/ 771]               blk.42.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 519/ 771]                 blk.42.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 520/ 771]                   blk.43.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 521/ 771]                 blk.43.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 522/ 771]              blk.43.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 523/ 771]            blk.43.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 524/ 771]                   blk.43.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 525/ 771]                 blk.43.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 526/ 771]                   blk.43.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 527/ 771]                 blk.43.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 528/ 771]               blk.43.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 529/ 771]               blk.43.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 530/ 771]               blk.43.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 531/ 771]                 blk.43.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 532/ 771]                   blk.44.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 533/ 771]                 blk.44.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 534/ 771]              blk.44.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 535/ 771]            blk.44.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 536/ 771]                   blk.44.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 537/ 771]                 blk.44.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 538/ 771]                   blk.44.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 539/ 771]                 blk.44.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 540/ 771]               blk.44.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 541/ 771]               blk.44.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 542/ 771]               blk.44.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 543/ 771]                 blk.44.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 544/ 771]                   blk.45.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 545/ 771]                 blk.45.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 546/ 771]              blk.45.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 547/ 771]            blk.45.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 548/ 771]                   blk.45.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 549/ 771]                 blk.45.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 550/ 771]                   blk.45.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 551/ 771]                 blk.45.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 552/ 771]               blk.45.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 553/ 771]               blk.45.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 554/ 771]               blk.45.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 555/ 771]                 blk.45.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 556/ 771]                   blk.46.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 557/ 771]                 blk.46.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 558/ 771]              blk.46.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 559/ 771]            blk.46.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 560/ 771]                   blk.46.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 561/ 771]                 blk.46.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 562/ 771]                   blk.46.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 563/ 771]                 blk.46.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 564/ 771]               blk.46.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 565/ 771]               blk.46.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 566/ 771]               blk.46.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 567/ 771]                 blk.46.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 568/ 771]                   blk.47.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 569/ 771]                 blk.47.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 570/ 771]              blk.47.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 571/ 771]            blk.47.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 572/ 771]                   blk.47.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 573/ 771]                 blk.47.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 574/ 771]                   blk.47.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 575/ 771]                 blk.47.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 576/ 771]               blk.47.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 577/ 771]               blk.47.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 578/ 771]               blk.47.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 579/ 771]                 blk.47.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 580/ 771]                   blk.48.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 581/ 771]                 blk.48.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 582/ 771]              blk.48.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 583/ 771]            blk.48.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 584/ 771]                   blk.48.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 585/ 771]                 blk.48.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 586/ 771]                   blk.48.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 587/ 771]                 blk.48.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 588/ 771]               blk.48.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 589/ 771]               blk.48.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 590/ 771]               blk.48.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 591/ 771]                 blk.48.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 592/ 771]                   blk.49.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 593/ 771]                 blk.49.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 594/ 771]              blk.49.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 595/ 771]            blk.49.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 596/ 771]                   blk.49.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 597/ 771]                 blk.49.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 598/ 771]                   blk.49.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 599/ 771]                 blk.49.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 600/ 771]               blk.49.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 601/ 771]               blk.49.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 602/ 771]               blk.49.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 603/ 771]                 blk.49.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 604/ 771]                   blk.50.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 605/ 771]                 blk.50.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 606/ 771]              blk.50.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 607/ 771]            blk.50.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 608/ 771]                   blk.50.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 609/ 771]                 blk.50.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 610/ 771]                   blk.50.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 611/ 771]                 blk.50.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 612/ 771]               blk.50.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 613/ 771]               blk.50.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 614/ 771]               blk.50.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 615/ 771]                 blk.50.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 616/ 771]                   blk.51.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 617/ 771]                 blk.51.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 618/ 771]              blk.51.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 619/ 771]            blk.51.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 620/ 771]                   blk.51.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 621/ 771]                 blk.51.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 622/ 771]                   blk.51.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 623/ 771]                 blk.51.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 624/ 771]               blk.51.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 625/ 771]               blk.51.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 626/ 771]               blk.51.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 627/ 771]                 blk.51.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 628/ 771]                   blk.52.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 629/ 771]                 blk.52.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 630/ 771]              blk.52.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 631/ 771]            blk.52.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 632/ 771]                   blk.52.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 633/ 771]                 blk.52.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 634/ 771]                   blk.52.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 635/ 771]                 blk.52.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 636/ 771]               blk.52.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 637/ 771]               blk.52.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 638/ 771]               blk.52.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 639/ 771]                 blk.52.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 640/ 771]                   blk.53.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 641/ 771]                 blk.53.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 642/ 771]              blk.53.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 643/ 771]            blk.53.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 644/ 771]                   blk.53.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 645/ 771]                 blk.53.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 646/ 771]                   blk.53.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 647/ 771]                 blk.53.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 648/ 771]               blk.53.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 649/ 771]               blk.53.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 650/ 771]               blk.53.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 651/ 771]                 blk.53.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 652/ 771]                   blk.54.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 653/ 771]                 blk.54.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 654/ 771]              blk.54.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 655/ 771]            blk.54.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 656/ 771]                   blk.54.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 657/ 771]                 blk.54.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 658/ 771]                   blk.54.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 659/ 771]                 blk.54.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 660/ 771]               blk.54.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 661/ 771]               blk.54.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 662/ 771]               blk.54.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 663/ 771]                 blk.54.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 664/ 771]                   blk.55.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 665/ 771]                 blk.55.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 666/ 771]              blk.55.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 667/ 771]            blk.55.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 668/ 771]                   blk.55.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 669/ 771]                 blk.55.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 670/ 771]                   blk.55.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 671/ 771]                 blk.55.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 672/ 771]               blk.55.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 673/ 771]               blk.55.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 674/ 771]               blk.55.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 675/ 771]                 blk.55.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 676/ 771]                   blk.56.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 677/ 771]                 blk.56.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 678/ 771]              blk.56.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 679/ 771]            blk.56.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 680/ 771]                   blk.56.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 681/ 771]                 blk.56.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 682/ 771]                   blk.56.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 683/ 771]                 blk.56.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 684/ 771]               blk.56.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 685/ 771]               blk.56.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 686/ 771]               blk.56.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 687/ 771]                 blk.56.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 688/ 771]                   blk.57.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 689/ 771]                 blk.57.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 690/ 771]              blk.57.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 691/ 771]            blk.57.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 692/ 771]                   blk.57.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 693/ 771]                 blk.57.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 694/ 771]                   blk.57.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 695/ 771]                 blk.57.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 696/ 771]               blk.57.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 697/ 771]               blk.57.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 698/ 771]               blk.57.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 699/ 771]                 blk.57.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 700/ 771]                   blk.58.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 701/ 771]                 blk.58.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 702/ 771]              blk.58.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 703/ 771]            blk.58.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 704/ 771]                   blk.58.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 705/ 771]                 blk.58.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 706/ 771]                   blk.58.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 707/ 771]                 blk.58.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 708/ 771]               blk.58.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 709/ 771]               blk.58.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 710/ 771]               blk.58.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 711/ 771]                 blk.58.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 712/ 771]                   blk.59.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 713/ 771]                 blk.59.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 714/ 771]              blk.59.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 715/ 771]            blk.59.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 716/ 771]                   blk.59.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 717/ 771]                 blk.59.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 718/ 771]                   blk.59.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 719/ 771]                 blk.59.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 720/ 771]               blk.59.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 721/ 771]               blk.59.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 722/ 771]               blk.59.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 723/ 771]                 blk.59.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 724/ 771]                   blk.60.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 725/ 771]                 blk.60.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 726/ 771]              blk.60.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 727/ 771]            blk.60.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 728/ 771]                   blk.60.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 729/ 771]                 blk.60.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 730/ 771]                   blk.60.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 731/ 771]                 blk.60.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 732/ 771]               blk.60.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 733/ 771]               blk.60.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 734/ 771]               blk.60.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 735/ 771]                 blk.60.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 736/ 771]                   blk.61.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 737/ 771]                 blk.61.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 738/ 771]              blk.61.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 739/ 771]            blk.61.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 740/ 771]                   blk.61.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 741/ 771]                 blk.61.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 742/ 771]                   blk.61.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 743/ 771]                 blk.61.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 744/ 771]               blk.61.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 745/ 771]               blk.61.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 746/ 771]               blk.61.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 747/ 771]                 blk.61.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 748/ 771]                   blk.62.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 749/ 771]                 blk.62.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 750/ 771]              blk.62.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 751/ 771]            blk.62.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 752/ 771]                   blk.62.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 753/ 771]                 blk.62.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 754/ 771]                   blk.62.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 755/ 771]                 blk.62.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 756/ 771]               blk.62.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 757/ 771]               blk.62.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 758/ 771]               blk.62.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 759/ 771]                 blk.62.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 760/ 771]                   blk.63.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 761/ 771]                 blk.63.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 762/ 771]              blk.63.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 763/ 771]            blk.63.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 764/ 771]                   blk.63.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 765/ 771]                 blk.63.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 766/ 771]                   blk.63.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 767/ 771]                 blk.63.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 768/ 771]               blk.63.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 769/ 771]               blk.63.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 770/ 771]               blk.63.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 771/ 771]                 blk.63.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "llama_model_quantize_impl: model size  = 62494.27 MB\n",
      "llama_model_quantize_impl: quant size  = 18926.01 MB\n",
      "\n",
      "main: quantize time = 243777.08 ms\n",
      "main:    total time = 243777.08 ms\n",
      "Unsloth: Conversion completed! Output location: /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"QwQ-Medical-COT-Tiny-GGUF\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8fee6-a2c5-46eb-ab28-917c905e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\")\n",
    "#model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34101af6-bc92-4e72-b432-a520b0dd63ce",
   "metadata": {},
   "source": [
    "创建完GGUF文件后如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dad9da-6b4b-4ff7-8bd4-596d6363eef9",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311173422135.png\" alt=\"image-20250311173422135\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4718d-a720-490e-8032-41edbbc9f16f",
   "metadata": {},
   "source": [
    "- 微调后模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec9592-a14a-43cf-b9f8-7e55fedebb9b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;为避免变量名称冲突，这里需要先重启Jupyter Kernel，然后微调模型保存和推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80bf42e-5f1a-47af-b94d-f5444aadaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b5b6b73-7e8c-4631-a777-18f5af25bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa3a395c-1999-445d-b55e-02a1b32d35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H800 PCIe. Num GPUs = 2. Max memory: 79.205 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e02ec6d16b14a53b1e020388ddc14d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/root/autodl-tmp/QwQ-Medical-COT-Tiny\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cc227-7af6-4bd8-aeda-25a10134e3cd",
   "metadata": {},
   "source": [
    "此时model就是读取进来模型就是微调后模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d18c6ebd-a376-4fc3-8ac0-4cf2732eb32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b918663c-436e-479e-bee4-36e1dab52af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/root/autodl-tmp/QwQ-Medical-COT-Tiny', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc617495-c550-403f-b97f-be89a22a4e89",
   "metadata": {},
   "source": [
    "然后设置为推理模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "816befad-34c7-45bb-a5b6-90420af09aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756eab7-47b0-47e3-b2bd-1ef5c43a6a8d",
   "metadata": {},
   "source": [
    "并设置提示词模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9749dac-be0e-422d-9cc3-bc63554d6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style_chat = \"\"\"请写出一个恰当的回答来完成当前对话任务。\n",
    "\n",
    "### Instruction:\n",
    "你是一名助人为乐的助手。\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d84917-674e-4a80-b283-ff25e57629dc",
   "metadata": {},
   "source": [
    "然后即可开始进行对话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85ccd98f-26e5-4702-ab8c-fed2ed3f227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，好久不见！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58a793be-d98a-4ef5-9360-6c97135277e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请写出一个恰当的回答来完成当前对话任务。\\n\\n### Instruction:\\n你是一名助人为乐的助手。\\n\\n### Question:\\n你好，好久不见！\\n\\n### Response:\\n<think>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prompt_style_chat.format(question, \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0495f257-ec7d-4acd-89a3-4628176f3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d903bbc3-cd14-4171-9ec7-4e2d1389337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d030ddf-8c50-4164-93af-32c2c86b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23adc604-57de-4f61-915d-2da54f7a18e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请写出一个恰当的回答来完成当前对话任务。\\n\\n### Instruction:\\n你是一名助人为乐的助手。\\n\\n### Question:\\n你好，好久不见！\\n\\n### Response:\\n<think>\\n嗯，用户发来“你好，好久不见！”，我需要回应。首先，应该表达同样的问候，比如“你好啊！”然后，用户提到“好久不见”，说明之前可能有段时间没联系了，应该回应说见到他们很高兴，或者问最近怎么样。比如可以说“很高兴又见到你！最近怎么样？”。这样既回应了对方的问候，又开启了进一步的对话，让用户有机会分享近况。另外，要保持友好和热情的语气，符合助人为乐助手的角色。可能还要注意不要问得太深入，保持自然和轻松。所以综合起来，回复应该是：“你好啊！很高兴又见到你，最近怎么样？有什么我可以帮你的吗？”这样既友好，又提供了帮助的可能，符合用户作为助手的设定。\\n</think>\\n\\n你好啊！很高兴又见到你，最近怎么样？有什么我可以帮你的吗？<|im_end|>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe45db40-eac0-4f73-b42a-e29582fda933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "嗯，用户发来“你好，好久不见！”，我需要回应。首先，应该表达同样的问候，比如“你好啊！”然后，用户提到“好久不见”，说明之前可能有段时间没联系了，应该回应说见到他们很高兴，或者问最近怎么样。比如可以说“很高兴又见到你！最近怎么样？”。这样既回应了对方的问候，又开启了进一步的对话，让用户有机会分享近况。另外，要保持友好和热情的语气，符合助人为乐助手的角色。可能还要注意不要问得太深入，保持自然和轻松。所以综合起来，回复应该是：“你好啊！很高兴又见到你，最近怎么样？有什么我可以帮你的吗？”这样既友好，又提供了帮助的可能，符合用户作为助手的设定。\n",
      "</think>\n",
      "\n",
      "你好啊！很高兴又见到你，最近怎么样？有什么我可以帮你的吗？<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a699bd0-928e-4d4d-9216-21006750f907",
   "metadata": {},
   "source": [
    "此时显存约20G左右：\n",
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311191243266.png\" alt=\"image-20250311191243266\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358b099-2f75-4fb3-b94c-8e64ddbefb63",
   "metadata": {},
   "source": [
    "- transformers推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc6511-bfd2-41ab-b14b-f27c96fb4fc2",
   "metadata": {},
   "source": [
    "为了避免重复读取占用显存，这里也可以先重启kernel，再执行以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed598c39-af8a-4e3b-92a7-5665985eac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3431071adbba483fb140fc1c84a5ac55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗯，用户跟我说“你好，好久不见！”，看起来挺开心的。我得先回应一下他的热情，毕竟好久没联系了，应该让他感受到我的热情和关心。然后，我得想一想他为什么会突然联系我，是不是遇到了什么问题呢？或者他只是想聊聊近况？这时候我得保持开放的态度，让他知道我很乐意帮忙或者倾听。\n",
      "\n",
      "接下来，我应该鼓励他多说一些，这样我们才能更好地了解他的需求。比如问他最近过得怎么样，或者有什么想分享的事情。不过，我得注意别太急，让他感觉自然一些。毕竟有时候人们可能不太愿意一开始就透露太多。\n",
      "\n",
      "另外，我得确保我的回答既友好又专业，不能太随便也不能太生硬。要让他觉得我们之间的交流既轻松又有效。可能的话，可以举几个例子，比如他可能需要帮助解决问题，或者只是想随便聊聊。这样他可以根据自己的情况选择合适的回应。\n",
      "\n",
      "最后，我得准备好根据他的回应进一步调整对话的方向。如果他提到具体的问题，我需要深入讨论；如果他只是想闲聊，那就保持轻松的氛围。总之，我的目标是让他感到舒适，并且愿意继续交流下去。\n",
      "</think>\n",
      "\n",
      "你好！好久不见，确实有些日子没联系了呢。最近过得怎么样？有什么新鲜事或者需要帮忙的吗？我很期待听听你的近况！\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/autodl-tmp/QwQ-Medical-COT-Tiny\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"你好，好久不见！\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc0cbd-51a1-4a84-9bdd-68ac60b6903e",
   "metadata": {},
   "source": [
    "能够看出对话正常。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cc29a-4e01-49ea-98b6-2857d38b9676",
   "metadata": {},
   "source": [
    "- ollama推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f805a-6f78-4248-af24-79db32d209cb",
   "metadata": {},
   "source": [
    "这里我们需要将创建的Q4_K_M模型权重单独拷贝一份，然后再编写ModelFile文件，便于ollama调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21eed7b-2e5f-4b76-a3db-ba9f4620a5de",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311200625855.png\" alt=\"image-20250311200625855\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f7252-d16a-41d5-aa04-bca10d78f3c7",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF\n",
    "mkdir ./unsloth_Q4\n",
    "cp unsloth.Q4_K_M.gguf ./unsloth_Q4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb44365-93c3-41ff-8572-9a0a9a650b77",
   "metadata": {},
   "source": [
    "然后创建ModelFile："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c07c4-9c86-4eae-a63c-a89a553a5893",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201030660.png\" alt=\"image-20250311201030660\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6a481-f615-4a00-ae1e-8962b44efd92",
   "metadata": {},
   "source": [
    "并输入如下内容："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b3b8e-1dbf-49fc-97ee-8f759b24b78b",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201546078.png\" alt=\"image-20250311201546078\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81136ad-1ec3-4cf1-bdd7-4581fa574c56",
   "metadata": {},
   "source": [
    "```bash\n",
    "FROM ./unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"\n",
    "请写出一个恰当的回答来完成当前对话任务。\n",
    "\n",
    "### Instruction:\n",
    "你是一名助人为乐的助手。\n",
    "\n",
    "### Question:\n",
    "{{ .Prompt }}\n",
    "\n",
    "### Response:\n",
    "<think>{{ .Response }}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER stop \"<|im_end|>\"\n",
    "PARAMETER stop \"<|end_of_text|>\"\n",
    "PARAMETER stop \"<|reserved_special_token_>\"\n",
    "PARAMETER temperature 1.5\n",
    "PARAMETER min_p 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdae976-eada-4fd3-8de5-9831dd8f08b0",
   "metadata": {},
   "source": [
    "保存并退出，然后进行模型注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0820d5-e10d-4229-b278-0df5ce264949",
   "metadata": {},
   "source": [
    "```bash\n",
    "ollama create unsloth_model -f /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth_Q4/ModelFile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f254a20-b633-4077-be40-a9cb56154a97",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201143237.png\" alt=\"image-20250311201143237\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38cd39-92e1-4cbc-96f7-0349f2e612dc",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201322617.png\" alt=\"image-20250311201322617\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33362c-bafe-4b0a-a518-db4feadfce76",
   "metadata": {},
   "source": [
    "然后即可开始调用了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfdb33d0-dc52-4220-b8a4-3f8bf9595e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "嗯，用户跟我说“你好，好久不见！”，听起来他们可能很久没联系我了。这说明我们之间有一段时间没有交流了，也许之前有过一些互动或者认识？这时候我应该先回应他们的问候，表达见到他们很高兴的感觉。\n",
      "\n",
      "然后，我需要确认一下用户现在的情况如何，是否一切顺利。这样可以让对话更自然地延续下去，同时也能让用户感到被关心和支持。接下来，可以问问用户有什么特别的事情想分享，这样既展示了我对他们的兴趣，也鼓励他们继续交流。\n",
      "\n",
      "最后，保持友好和亲切的态度很重要，让用户觉得轻松自在，愿意进一步沟通。总之，我需要确保回应既热情又体贴，帮助我们重新建立联系，并开启一个愉快的对话。\n",
      "</think>\n",
      "你好！是啊，确实好久不见了呢。最近过得怎么样？有什么特别的事情想分享吗？很高兴再次见到你！\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',  # required but ignored\n",
    ")\n",
    "prompt = \"你好，好久不见！\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='unsloth_model',\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47079398-7559-4046-9ae5-adee481cc487",
   "metadata": {},
   "source": [
    "- vLLM推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e702afc-4212-4a59-b2a1-5c95960d1986",
   "metadata": {},
   "source": [
    "相比之下，vLLM的调用过程就简单多了，只需要在后台开启服务即可："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58933bd-9ae6-4022-a4fe-76e82cd9caa4",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201355019.png\" alt=\"image-20250311201355019\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f16162-296f-4d20-852d-787a2d9acd42",
   "metadata": {},
   "source": [
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve /root/autodl-tmp/QwQ-Medical-COT-Tiny --tensor-parallel-size 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8265a3d-6cd5-4511-ac7c-9ae80e8639fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，我现在要处理用户的信息。用户说：“你好，好久不见！”。首先，我要分析用户此时可能的情绪和需求。\n",
      "\n",
      "看用户的问候，明显是带着亲切感和久别重逢的感觉。这种情况下，用户可能在表达对过去交流的想念，或者希望展开一段愉快的对话。看起来用户的心情是积极而温暖的，这也意味着我应该保持同样的热情回应。\n",
      "\n",
      "接下来，我需要用合适的语气来回应。根据用户友好和亲切的问候，我应该以相同的方式回复，这样能营造出融洽的氛围。同时，融入一些轻松的幽默元素，让对话更加生动有趣，这通常能增强互动性和亲切感。\n",
      "\n",
      "为了确保用户能得到最佳的回应，我应该使用简练而真诚的语言。这样不仅能让回答显得自然，还能让用户感到被理解和关心。同时，适当的幽默可以进一步提升对话的轻松感，让用户感到更加自在。\n",
      "\n",
      "现在，结合我之前提出的几点思考，来写出回应：“哈哈，好久不见！看来你找到我了。很高兴和你重逢，有什么新鲜事要分享吗？或者你有什么需要帮忙的？” 这个回答亲切且有助于推动对话向前发展，符合用户的需求。\n",
      "\n",
      "最后，我会再检查一下，确保我的回应确实是积极的，并且能够延续与用户的愉快交流。这样，我们就能够一起享受这场轻松而愉快的对话了。\n",
      "</think>\n",
      "\n",
      "哈哈，好久不见！看来你找到我了。很高兴和你重逢，有什么新鲜事要分享吗？或者你有什么需要帮忙的？\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "prompt = \"你好，好久不见！\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"/root/autodl-tmp/QwQ-Medical-COT-Tiny\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7bc26-8d7a-4388-aa4b-4c48b6875f21",
   "metadata": {},
   "source": [
    "### 三、完整高效微调实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446a0c5-c8cf-45ae-a069-8a4ee40f9e60",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后，我们尝试带入全部数据进行高效微调，以提升模型微调效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b634ea8-b9a1-4b80-adec-8ca219f862b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b173a851-3466-4596-903a-3e591408e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5cdcf-be2a-4c69-96b7-66490e6029e1",
   "metadata": {},
   "source": [
    "此时读取全部数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3d9b24-29e0-4868-afb4-527ae245d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350eda58abfe4178bb86ee7308aaf3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<｜end▁of▁sentence｜>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train\",trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4eb8209-4f35-4551-8a8b-35aecb0eccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7dce0-84cd-46e2-b0a7-a05e3b81b211",
   "metadata": {},
   "source": [
    "这里设置epoch为3，遍历3次数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4739bb1a-0d51-48d2-beb3-0d6449e7ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d1143312447a0ac71a3f7b0308549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs = 3,\n",
    "        warmup_steps=5,\n",
    "        # max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8576d5-8fb1-4b5d-8707-24703ff79945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 25,371 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 9,513\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='389' max='9513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 389/9513 13:44 < 5:24:01, 0.47 it/s, Epoch 0.12/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87f0b5-c6cc-4885-b19d-bc3b3cd1cfae",
   "metadata": {},
   "source": [
    "这里总共训练约15个小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff611dd-e6be-4066-97d4-b288711465bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9513, training_loss=1.0824475168592858, metrics={'train_runtime': 20193.217, 'train_samples_per_second': 3.769, 'train_steps_per_second': 0.471, 'total_flos': 2.7936033274397737e+18, 'train_loss': 1.0824475168592858, 'epoch': 2.9992117294655527})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57354364-e766-4094-b3f2-03d9f736110f",
   "metadata": {},
   "source": [
    "带入两个问题进行测试，均有较好的回答效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7893ec0-c557-4347-91f0-af1a28163451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think this through step by step. We've got a 61-year-old woman who's been dealing with involuntary urine loss whenever she does something like coughing or sneezing. That sounds like stress urinary incontinence, which usually means there's some kind of leakage when there's increased pressure in the abdomen. Now, the fact that she doesn't have any leakage at night is interesting. That's a big hint because it suggests that the problem isn't about bladder control or retention, since there's no issue when lying down.\n",
      "\n",
      "Now, let's consider the Q-tip test. This test is a clever way to check for urethral mobility. If the Q-tip moves a lot, it means the urethra is mobile, which is a typical sign of stress incontinence. So, we're probably looking at a condition where the urethra isn't staying closed under pressure.\n",
      "\n",
      "Now, if we were to do a cystometry on her, we'd be checking a few things. First, there's the residual volume. Since she's not leaking urine at night, her bladder should be empty, so we'd expect the residual volume to be normal. Then, there's the detrusor contractions. Stress incontinence doesn't usually involve the detrusor muscle contracting abnormally. So, we'd expect normal or maybe slightly increased contractions, but nothing out of the ordinary.\n",
      "\n",
      "So, putting it all together, if we do the cystometry, we'd likely see a normal residual volume and normal detrusor contractions. That fits with everything we know about her symptoms and the Q-tip test results. Everything lines up with stress urinary incontinence, confirming that our conclusion makes sense.\n",
      "</think>\n",
      "Based on the information provided, the 61-year-old woman is likely experiencing stress urinary incontinence, as suggested by the involuntary urine loss during activities like coughing or sneezing, coupled with the fact that she doesn't experience leakage at night. During a cystometry exam, we would expect the following findings:\n",
      "\n",
      "1. **Residual Volume:** This would be normal. Since she does not experience leakage at night, her bladder is likely empty at bedtime, indicating a normal post-void residual volume.\n",
      "\n",
      "2. **Detrusor Contractions:** These would also be normal or possibly slightly increased. Stress urinary incontinence typically does not involve abnormal detrusor contractions, as the issue is more related to the closure mechanism of the urethra under increased abdominal pressure rather than bladder control or contractions.\n",
      "\n",
      "Thus, cystometry would most likely reveal a normal residual volume and normal detrusor contractions, consistent with stress urinary incontinence.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8bba1c2-a7c7-4e82-9795-48a117c70c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's think about this. The patient is having sudden chest pain that's shooting to their neck and left arm. That's pretty classic for something like a myocardial infarction, right? I mean, the pain spreading like that usually points to a big problem in the heart's supply.\n",
      "\n",
      "Now, considering the heart's coronary arteries, I know the left anterior descending artery, or LAD, is a big player. It's like the main highway for blood to reach the front wall of the heart. If there's a blockage there, it can definitely cause pain that radiates to the neck and arm.\n",
      "\n",
      "Then, there's the right coronary artery, or RCA, which supplies the right side of the heart and can affect the inferior wall of the heart. But wait, the pain pattern here seems to be more on the left side, so maybe the RCA is less likely.\n",
      "\n",
      "The patient has hypercholesterolemia and coronary artery disease. These conditions put them at risk for atherosclerosis, which can lead to blockages in the coronary arteries. The LAD is commonly involved in such scenarios, especially when the pain spreads to the neck and arm.\n",
      "\n",
      "Also, the elevated troponin I levels and tachycardia are strong signals that something serious is happening in the heart. These are usually seen in myocardial infarctions. Given the pain pattern and the patient's risk factors, the LAD seems like the most likely culprit here.\n",
      "\n",
      "So, when I put all this together, it really seems like the left anterior descending artery is the most likely artery involved in this situation. It just fits with the classic presentation of anterior myocardial infarction. Yeah, I'm pretty confident about that.\n",
      "</think>\n",
      "Based on the presentation of sudden-onset chest pain radiating to the neck and left arm, along with the patient's history of hypercholesterolemia and coronary artery disease, the most likely coronary artery involved is the left anterior descending (LAD) artery. This artery supplies the front wall of the heart, and a blockage here can cause the classic symptoms described. The elevated troponin I levels and tachycardia further support the likelihood of a myocardial infarction, with the LAD being a common site for such events.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "question = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
